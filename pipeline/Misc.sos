#!/usr/bin/env sos-runner
#fileformat=SOS1.0

# Various calls to commands to accomplish some workflow
# When some workflow gets complicated they will be separated into dedicated files


parameter: project_name = "GTEx7"

[genotype_stats_1]
# making genotype summary statistics via vcftools
parameter: workdir = None
depends: executable("vcftools")
input: pattern = "{name}.vcf.gz"
output: expand_pattern("${CONFIG['wd']}/${name!b}.imiss"), expand_pattern("${CONFIG['wd']}/${name!b}.lmiss")
task: workdir = workdir
run: 
  vcftools --gzvcf ${input} --out ${output[0]!n} --missing-indv
  vcftools --gzvcf ${input} --out ${output[1]!n} --missing-site

[genotype_stats_2]
# making genotype summary statistics plot
parameter: workdir = None
depends: Py_Module("seaborn")
input: group_by = 1, pattern = "{name}.{ext}"
output: expand_pattern("{_name}.{_ext}.pdf")
task: workdir = workdir
python:
import matplotlib.pyplot as plt, seaborn as sns, pandas as pd
fig, axs = plt.subplots(ncols=2)
data = pd.read_csv(${_input!r}, sep = '\t')
sns.distplot(data["F_MISS"], ax = axs[0], kde = False)
sns.violinplot(data["F_MISS"], ax = axs[1])
axs[0].set_title(${_input!br}.split('.')[-1])
fig.savefig(${_output!r})

[rnaseq_1]
# Quantile normalization of RNA-seq data
# 1. expression values are quantile normalized to the average empirical distribution observed across samples
# 2. for each gene, expression values are inverse quantile normalized to a standard normal distribution across samples
# genes are selected based on expression thresholds of >0.1 RPKM in >10 samples and >5 reads in >10 samples
# input are rpkm file (for normalization), count file (for QC) and vcf file (for removing samples that do not have genotypes)
parameter: rpkm_cutoff = 0.1
parameter: read_cutoff = 5
parameter: sample_cutoff = 10
parameter: script = os.path.abspath("../src/normalize_expression.py")
parameter: workdir = None
output: "${workdir}/${input[0]!nnb}.qnorm.std.flat.h5", "${workdir}/${input[0]!nnb}.qnorm.flat.h5", "${workdir}/${input[0]!nnb}.qnorm.std.h5", "${workdir}/${input[0]!nnb}.qnorm.h5"
task: workdir = workdir
run:
  python ${script} ${input[0]} ${input[1]} ${input[2]} ${input[3]} ${input[0]!nnb} --expression_threshold ${rpkm_cutoff} --count_threshold ${read_cutoff} --min_samples ${sample_cutoff}

[rnaseq_2]
# PEER analysis
# There are a number of configuable parameters in this script
# Using default values for now. Can be changed here or not depending on the analyst
depends: R_library('rhdf5'), R_library('peer')
parameter: tissues = get_output("h5ls ${input[3]} | awk '{print $1}'").strip().split('\n')
parameter: workdir = None
input: for_each = ['tissues']
output: "${workdir}/${_tissues}_PEER_covariates.txt", "${workdir}/${_tissues}_PEER_alpha.txt", "${workdir}/${_tissues}_PEER_residuals.txt"
task: workdir = workdir, walltime = "20:00:00", cores = 1, mem = "4G"
R:

expr.h5 = ${input[3]!r}
prefix = ${_tissues!r}
alphaprior_a=0.001
alphaprior_b=0.01
epsprior_a=0.1
epsprior_b=10
max_iter=1000

library(peer, quietly=TRUE)  # https://github.com/PMBio/peer
library(rhdf5, quietly=TRUE)

WriteTable <- function(data, filename, index.name) {
	datafile <- file(filename, open = "wt")
	on.exit(close(datafile))
	header <- c(index.name, colnames(data))
	writeLines(paste0(header, collapse="\t"), con=datafile, sep="\n")
	write.table(data, datafile, sep="\t", col.names=FALSE, quote=FALSE)
}

loadTable <- function(filename, group, auto_transpose = FALSE) {
  obj <- h5read(filename, group)
  dat <- obj$block0_values
  rownames(dat) <- obj$axis0
  colnames(dat) <- obj$axis1
  if (ncol(dat) > nrow(dat) && auto_transpose) dat <- t(dat)
  return(dat)
}

getNumPeer <- function(ss) {
  if (ss<150) return (15)
  else if (ss >=150 && ss < 250) return(30)
  else return(35)
}

cat("PEER: loading expression data ... ")
# rows are number of samples. columns are number of genes
M <- as.matrix(loadTable(expr.h5, "/${_tissues}"))
n = getNumPeer(nrow(M))
cat("done.\n")

# run PEER
cat(paste0("PEER: estimating hidden confounders (", n, " for tissue ", prefix , ")\n"))
model <- PEER()
invisible(PEER_setNk(model, n))
invisible(PEER_setPhenoMean(model, M))
invisible(PEER_setPriorAlpha(model, alphaprior_a, alphaprior_b))
invisible(PEER_setPriorEps(model,epsprior_a, epsprior_b))
invisible(PEER_setNmax_iterations(model, max_iter))
# if(!is.null(covs)) {
#   invisible(PEER_setCovariates(model, covs))
# }
time <- system.time(PEER_update(model))

X <- PEER_getX(model)  # samples x PEER factors
A <- PEER_getAlpha(model)  # PEER factors x 1
R <- t(PEER_getResiduals(model))  # genes x samples

# add relevant row/column names
c <- paste0("InferredCov",1:ncol(X))
rownames(X) <- rownames(M)
colnames(X) <- c
rownames(A) <- c
colnames(A) <- "Alpha"
A <- as.data.frame(A)
A$Relevance <- 1.0 / A$Alpha
rownames(R) <- colnames(M)
colnames(R) <- rownames(M)

# write results
cat("PEER: writing results ... ")
WriteTable(t(X), paste0(prefix, "_PEER_covariates.txt"), "ID")  # format(X, digits=6)
WriteTable(A, paste0(prefix, "_PEER_alpha.txt"), "ID")
WriteTable(R, paste0(prefix, "_PEER_residuals.txt"), "ID")
cat("done.\n")

[umich_to_plink_1]
# UMich imputed VCF to plink format
# Fix multi-allelic sites
# By changing duplicate SNP ID

parameter: filter = 1
input: group_by = 'single'
output: "${_input!nn}.bed"
task:
run:
  if [[ ${filter} ]]; then
     zcat ${_input} | awk '{if ($7 == "PASS") {print $2}}' > ${_input!nn}.exclude_id
     plink --vcf ${_input} --out ${_output!nn} --exclude ${_input!nn}.exclude_id --make-bed
     rm -f ${_input!nn}.exclude_id
  else
     plink --vcf ${_input} --out ${_output!nn} --make-bed
  fi

python:

from collections import Counter
import numpy as np

dat = np.genfromtxt('${_output!n}.bim', dtype = '<U48')
counter = Counter()
dup_list = dat[:,1]
deduped = []
for name in dup_list:
    new = name + '_' + str(counter[name]) if counter[name] else name
    counter.update({name: 1})
    deduped.append(new)
dat[:,1] = np.array(deduped)
np.savetxt('${_output!n}.bim', dat, delimiter = '\t', fmt = '%s')

[umich_to_plink_2]
# Merge all files
# And fix family ID (replace by ancestry coding)
parameter: phenotype = "${CONFIG['phenotype']!a}"
depends: phenotype
output: "${workdir}/${project_name}.bed"
task: workdir = workdir
run:
  echo ${input!n} | sed 's/ /\n/g' | grep -v chrX > plink.merge-list
  plink --bfile ${input[0]!n} --merge-list plink.merge-list --out ${output!n} --chr 1-22 --make-bed

python:
import pandas as pd
reference = pd.read_csv(${phenotype!r}, dtype = str, usecols = (0,4), sep = '\t')
reference['RACE'] = 'POP' + reference['RACE'].astype(str)
fam = pd.read_csv("${output!n}.fam", header = None, sep = ' ',
                  names = ['fid', 'sid', 'pid', 'mid', 'sex', 'phen'])
dat = pd.merge(reference, fam, left_on = 'SUBJID', right_on = "sid")
dat.drop(['SUBJID', 'fid'], axis=1, inplace = True)
dat.to_csv("${output!n}.fam", sep = ' ', header = False, index = False)

[variants_filter]
# Filter by MAF, HWE etc
parameter: maf = 0.01
parameter: mac = 10
output: "${input!n}.filtered.bed"
task: workdir = workdir
run:
  plink --bfile ${input!n} --maf ${maf} --mac ${mac} --make-bed --out ${output!n}

[LD_pruning_1]
# Mark independent variants
parameter: workdir = None
parameter: pairwise_ld_param = '50 5 0.2'
parameter: maf = 0.1
depends: executable('plink')
input: pattern = '{name}.{ext}'
output: expand_pattern('{name}.prune.in')
task: workdir = workdir 
run:
  plink --bfile ${input!n} \
        --allow-no-sex \
        --maf ${maf} \
        --indep-pairwise ${pairwise_ld_param} \
        --out ${output!nn}
        
[LD_pruning_2]
# Select independent common variants
parameter: workdir = None
depends: executable('plink')
input: pattern = '{name}.prune.in'
output: expand_pattern('{name}.prune.bed')
task: workdir = workdir 
run:
  plink --bfile ${input!nn} \
        --extract ${input} \
        --no-sex --no-pheno --no-parents \
        --make-bed \
        --out ${output!n}

[global_ancestry_1]
# global ancestry analysis via KING
parameter: workdir = None
depends: executable('king')
input: pattern = "{name}.{ext}"
output: expand_pattern('{name}.pc.ped')
task: workdir = workdir
run:
  king -b ${input!n}.bed --pca 20 --prefix ${input!n}.
  # Would be much faster if I use MDS
  # But I use PCA to be more comparable to GTEx official results
  # king -b ${input!n}.bed --mds --prefix ${input!n}.

[global_ancestry_2]
# scatter plot for global ancestry analysis result
# R plotly implementation
parameter: workdir = None
depends: R_library("plotly")
input: pattern = "{name}.ped" 
output: expand_pattern('{name}.html')
task: workdir = workdir 
R: 
   library(plotly)
   dat <- read.table(${input!r}, sep = ' ')
   for (i in 1:(ncol(dat) - 6 - 1)) {
       title <- paste(${input!bnr}, "PC", i, "vs", "PC", i + 1, sep = "_")
       p <- plot_ly(dat, x = dat[, 6 + i], y = dat[, 7 + i], text = dat[,2], color = dat[,1],
                    mode = "markers", type = 'scatter', visible = 'legendonly') %>% layout(title = title)
       htmlwidgets::saveWidget(as.widget(p), paste0(title, ".html"))
   }
run: 
   foo () {
     echo '<html><body>'
     sed 's/^.*/<a href="&">&<\/a><br\/>/'
     echo '</body></html>'
   }
   mkdir -p ${input!n}; mv ${input!bn}*.html ${input!n}
   ls ${input!n}/*.html | foo > ${output}

[fastqtl_1]
# FastQTL pipeline
