#!/usr/bin/env sos-runner
# vim: set filetype=python: set expandtab : ts=4:
#fileformat=SOS1.0

# This script provides routines for file conversions and data transformations

parameter: project_name = 'GTEx'

[vcf_by_chrom]
# split vcf by chroms
parameter: workdir = None
parameter: chrom = [x + 1 for x in range(22)] + ['X'] # there is in fact no Y chrom
input: pattern = "{name}.vcf.gz", for_each = ['chrom']
output: expand_pattern("${CONFIG['wd']}/${name!b}.chr{_chrom}.vcf.gz")
task: workdir = workdir
run:
  tabix ${input} ${_chrom} --print-header | bgzip > ${_output}

[RNASeq_to_HDF5]
# Convert RNASeq data to HDF5
parameter: workdir = None
parameter: dtype = 'np.uint32'
output: "${input[0]!nb}.hdf5"
task: workdir = workdir
python:
import pandas as pd
import numpy as np
import re, os
def load_data(fdata, fsample, dtype = np.float32):
    '''First col of expression data is ENCODE gene name, 2nd col is HUGO name'''
    head = pd.read_csv(fdata, skiprows = 2, sep = '\t', nrows = 1)
    dt = {'Description': str, 'Name': str}
    dt.update({x: dtype for x in head.columns if x not in dt})
    data = pd.read_csv(fdata, compression='gzip', skiprows=2, 
                       index_col=0, header=0, dtype = dt, sep='\t').drop('Description', 1)
    samples = pd.read_csv(fsample, dtype=str, delimiter='\t', header=0)
    sample_dict = {}
    for row in samples[['SAMPID', 'SMTSD']].values:
        if row[1] not in sample_dict:
            sample_dict[row[1]] = []
        if row[0] in data.columns:
            sample_dict[row[1]].append(row[0])
    return data, dict((re.sub("[\W\d]+", "_", k.strip()).strip('_'), v) for k, v in sample_dict.items() if len(v))

data, sample = load_data(${input[0]!r}, ${input[1]!r}, dtype = ${dtype})
data = {k: data.loc[:, sample[k]] for k in sample}
if os.path.isfile(${output!r}):
    os.remove(${output!r})
for k in data:
    data[k].to_hdf(${output!r}, k, mode = 'a', complevel = 9, complib = 'zlib')

[umich_to_plink_1, broad_to_plink_1]
# VCF to plink format
input: group_by = 'single'
output: "${_input!nn}.a2flipped.bed"
task: workdir = workdir
run:
  plink --vcf ${_input} --out ${_output!n} --make-bed

[umich_to_plink_2]
# Fix multi-allelic sites
# By changing duplicate SNP ID
input: group_by = 'single'
output: "${_input!n}.bim"
task: workdir = workdir
python:

from collections import Counter
import numpy as np

dat = np.genfromtxt(${_output!r}, dtype = '<U48')
counter = Counter()
dup_list = dat[:,1]
deduped = []
for name in dup_list:
    new = name + '_' + str(counter[name]) if counter[name] else name
    counter.update({name: 1})
    deduped.append(new)
dat[:,1] = np.array(deduped)
np.savetxt(${_output!r}, dat, delimiter = '\t', fmt = '%s')

[umich_to_plink_3]
# VCF to plink format
input: group_by = 'single'
output: "${_input!nn}.bed"
task: workdir = workdir
run:
  paste <(cut -f2 ${_input!n}.bim) <(zcat ${_input!nn}.vcf.gz | grep -v "^#" | cut -f4) > ${_input!nn}.a2
  plink --bfile ${_input!n} \
        --a2-allele ${_input!nn}.a2 2 1 '#' \
        --out ${_output!n} --make-bed
  rm -f ${_input!nn}.a2

[broad_to_plink_2]
# Remove indel sites
input: group_by = 'single'
output: "${_input!n}.snp_only.bed"
task: workdir = workdir
run:
  awk '{if ((length($5) > 1 ) || (length($6) > 1)) {print $2}}' ${_input!n}.bim > ${_input!n}.exclude_id
  plink --bfile ${_input!n} --out ${_output!n} --make-bed \
        --exclude ${_input!n}.exclude_id \
  rm -f ${_input!n}.exclude_id

[umich_to_plink_4, broad_to_plink_3]
# Merge all files
output: "${workdir}/${project_name}.bed"
task: workdir = workdir
run:
  echo ${input!n} | sed 's/ /\n/g' | grep -v chrX > plink.merge-list
  plink --bfile ${input[0]!n} --merge-list plink.merge-list --out ${output!n} --chr 1-22 --make-bed
  rm plink.merge-list

[variants_filter_1]
# Exclude imputed variants
parameter: include = None
output: "${input!n}.genotyped.bed"
task: workdir = workdir
python:
import pandas as pd
imputed = pd.read_csv("${input!n}.bim", dtype = str, usecols = (0,1,3), header = None, sep = '\t')
genotyped = pd.read_csv("${include!n}.bim", dtype = str, usecols = (0,1,3), header = None, sep = '\t')
imputed[0] = imputed[0] + '_' + imputed[3]
imputed.drop([3], axis = 1, inplace = True)
genotyped[0] = genotyped[0] + '_' + genotyped[3]
genotyped.drop([1,3], axis = 1, inplace = True)
res = pd.merge(imputed, genotyped, how='inner', on=[0,0])
res[1].to_csv("${input!n}.extract_id", header = False, index = False)

run:
  plink --bfile ${input!n} --out ${output!n} --extract ${input!n}.extract_id --make-bed
  rm -f ${input!n}.extract_id

[variants_filter_2]
# Filter by MAF, HWE etc
parameter: maf = 0.01
parameter: mac = 10
output: "${input!n}.filtered.bed"
task: workdir = workdir
run:
  plink --bfile ${input!n} --maf ${maf} --mac ${mac} --make-bed --out ${output!n}

[plink_to_hdf5_batch]
# Convert plink genotype to HDF5 in batches of genes
# Only autosomal genes are considered
# cis: ${cis} up/downstream from TSS of each gene
depends: Py_Module("pandas-plink")
parameter: ann = None
parameter: workdir = None
parameter: cis = 1000000
parameter: resume = 1
output: "${input!n}.cis.h5"
# task: workdir = workdir
python:
import os
import pandas as pd, numpy as np
from pandas_plink import read_plink

keys = []
if os.path.isfile(${output!r}):
   if ${resume} == 1:
      print("loading existing database ${output!b}")
      keys = pd.HDFStore(${output!r}).keys()
      print("{} existing genes will be skipped".format(len(keys)))
   else:
      os.remove(${output!r})

(bim, fam, bed) = read_plink(${input!nr})
ann = [x.strip().split() for x in open(${ann!ar}).readlines()]
n = len(ann)
empty_genes = []
for i, line in enumerate(ann):
    if(i % 100 == 0):
        print('[percent completed] %.2f' % ((float(i)/n)*100))
    if line[0] not in list(map(str, [x+1 for x in range(22)])):
        continue
    if '/chr{}/{}'.format(line[0], line[3]) in keys:
        continue
    snps = bim.query("chrom == '{}' and (pos >= {} and pos <= {})".\
                            format(line[0], int(line[1]) - ${cis}, int(line[1]) + ${cis}))
    if (snps.empty):
        empty_genes.append(line[3])
        continue
    X = pd.DataFrame(2 - bed[snps.i,:].compute(), 
                     index = [':'.join((x.split('_', 1)[0], y,z)) for x, y, z in zip(snps.snp, snps.a1, snps.a0)], 
                     columns = fam.iid, dtype = np.uint8)
    X.to_hdf(${output!r}, 'chr{}/{}'.format(line[0], line[3]), mode = 'a', complevel = 9, complib = 'zlib')
with open("${output!n}.empty_genes", 'w') as f:
     f.write('\n'.join(empty_genes))