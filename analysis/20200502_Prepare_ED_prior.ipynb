{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Multivariate EBNM based prior for M&M\n",
    "\n",
    "Here for the simulation benchmark we prepare mixture prior based on a mulrivariate Emperical Bayes Normal Mean model (previously we use Extreme Deconvolution for the task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Here is the analysis plan:\n",
    "\n",
    "1. Identify up to 20K genes where there is complete phenotype data to make a good / realistic residual variance estimate via FLASH\n",
    "2. Simulate 20K data under my phenotypic models (the latest DSC benchmark setting) and generate sumstats for them ; bhat and sbhat\n",
    "3. For each data-set, take the strongest gene-snp pair as the strong set\n",
    "4. Also select from each data-set perhaps 4 \"random\" gene-snp pair.\n",
    "5. then try to run your estimate of Vhat to get Vhat first, and run Yunqi / Peter's ED\n",
    "\n",
    "In GTEx we have >35K genes. The reason we want to try using 20K is that 20K seems to have enough information learning about the pattern of sharing between conditions. \n",
    "\n",
    "But we \"cheat\" a bit by simulating under identity residual variance for all genes, and fit EBNM assuming residual variance is identity, too; or just estimating a global residual variance. This makes the problem easier. Because in practice residual can be different (though maybe similar!) for different genes.\n",
    "\n",
    "So the simplified plan is to only do 2~5 with 2 using just identity matrix for residual variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('/project2/mstephens/gaow/mvarbvs/dsc/mnm_prototype/mnm_sumstats')\n",
    "parameter: model = 'artificial_mixture_identity' # 'gtex_mixture_identity'\n",
    "parameter: per_chunk = 200\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project2/mstephens/gaow/mvarbvs/dsc/mnm_prototype/mnm_sumstats"
     ]
    }
   ],
   "source": [
    "%cd /project2/mstephens/gaow/mvarbvs/dsc/mnm_prototype/mnm_sumstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Get top gene-SNP and random gene-SNP pairs per gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# extract data for MAHS from summary stats\n",
    "[1]\n",
    "parameter: seed = 999\n",
    "parameter: n_random = 4\n",
    "input: glob.glob(f'{cwd}/{model}/*.rds'), group_by = per_chunk\n",
    "output: f\"{cwd}/{model}/cache/{model}_{_index+1}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    set.seed(${seed})\n",
    "    matxMax <- function(mtx) {\n",
    "        max_idx <- which.max(mtx)\n",
    "        colmn <- max_idx %/% nrow(mtx) + 1\n",
    "        row <- max_idx %% nrow(mtx)\n",
    "        return( matrix(c(row, colmn), 1))\n",
    "    }\n",
    "    remove_rownames = function(x) {\n",
    "        for (name in names(x)) rownames(x[[name]]) = NULL\n",
    "        return(x)\n",
    "    }\n",
    "    extract_one_data = function(infile, n_random) {\n",
    "        # If cannot read the input for some reason then let it go. I dont care losing one.\n",
    "        dat = tryCatch(readRDS(infile)$sumstats, error = function(e) return(NULL))\n",
    "        if (is.null(dat)) return(NULL)\n",
    "        z = abs(dat$bhat/dat$sbhat)\n",
    "        max_idx = matxMax(z)\n",
    "        strong = list(bhat = dat$bhat[max_idx[1],,drop=F], sbhat = dat$sbhat[max_idx[1],,drop=F])\n",
    "        if (max_idx[1] == 1) {\n",
    "            sample_idx = 2:nrow(z)\n",
    "        } else if (max_idx[1] == nrow(z)) {\n",
    "            sample_idx = 1:(max_idx[1]-1)\n",
    "        } else {\n",
    "            sample_idx = c(1:(max_idx[1]-1), (max_idx[1]+1):nrow(z))\n",
    "        }\n",
    "        random_idx = sample(sample_idx, n_random, replace = T)\n",
    "        random = list(bhat = dat$bhat[random_idx,,drop=F], sbhat = dat$sbhat[random_idx,,drop=F])\n",
    "        return(list(random = remove_rownames(random),  strong = remove_rownames(strong)))\n",
    "    }\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else if (is.null(one_data)) {\n",
    "          return(res)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      res = merge_data(res, extract_one_data(f, ${n_random}))\n",
    "    }\n",
    "    saveRDS(res, ${_output:r})\n",
    "  \n",
    "[2]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd}/{model}.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\"\n",
    "    merge_data = function(res, one_data) {\n",
    "      if (length(res) == 0) {\n",
    "          return(one_data)\n",
    "      } else {\n",
    "          for (d in names(one_data)) {\n",
    "              for (s in names(one_data[[d]])) {\n",
    "                  res[[d]][[s]] = rbind(res[[d]][[s]], one_data[[d]][[s]])\n",
    "              }\n",
    "          }\n",
    "          return(res)\n",
    "      }\n",
    "    }\n",
    "    res = list()\n",
    "    for (f in c(${_input:r,})) {\n",
    "      res = merge_data(res, readRDS(f))\n",
    "    }\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "To run it:\n",
    "\n",
    "```\n",
    "for m in artificial_mixture_identity gtex_mixture_identity; do \n",
    "    sos run analysis/20200502_Prepare_ED_prior.ipynb --model $m -c midway2.yml -q midway2\n",
    "done\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.20.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
