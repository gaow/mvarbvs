{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular QTL workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%revisions -s -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Molecular QTL data from [Yang et al (2016) Science](http://eqtl.uchicago.edu/jointLCL/). Input are genotypes of ~100 YRI samples with their molecular QTL data measured in LCL.\n",
    "\n",
    "- alternative splicing (AS) data is of the primary interest here.\n",
    "\n",
    "### Genotypes\n",
    "\n",
    "[Genotype data for YRI](http://eqtl.uchicago.edu/jointLCL/genotypesYRI.gen.txt.gz) is the conventional VCF format but has dosage for genotypes.\n",
    "\n",
    "### Phenotypes\n",
    "\n",
    "Phenotype data has format:\n",
    "\n",
    "```\n",
    "#Chr\tstart\tend\tID\t18486\t18487\t18488\t18489\t18498\t18499\n",
    "chr1\t880180\t880422\tchr1:880180:880422:clu_15502\t0.201694364955\t0.665990212763\t-1.21881815589\t-0.342480185427\t0.165404160483\t-1.58524292941\n",
    "```\n",
    "\n",
    "The first 4 columns are genomic coordinates info. Others are molecular QTL in samples.\n",
    "\n",
    "We analyze:\n",
    "\n",
    "1. [Alternative splicing](http://eqtl.uchicago.edu/jointLCL/qqnorm_ASintron_RNAseqGeuvadis.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis plan\n",
    "\n",
    "- For each analysis unit (gene, or intron cluster for AS), get the 1MB up/down-stream variants in genotypes\n",
    "- Remove top phenotype PC from phenotype data\n",
    "- Fine-mapping using various methods. SuSiE for starters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run 20180704_MolecularQTL_Workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  SuSiE\n",
      "  index_vcf\n",
      "  R_loader\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd /home/gaow/GIT/LargeFiles/finemap_workflow_output (as path)\n",
      "                        Specify work directory\n",
      "  --x-data . (as path)\n",
      "                        X data, the genotype VCF file path\n",
      "  --y-data . (as path)\n",
      "                        Y data, the phenotype file paths\n",
      "  --h5-data . (as path)\n",
      "                        HDF5 file, merged X and Y data\n",
      "\n",
      "Sections\n",
      "  SuSiE_1:              PCA on phenotype and remove top PCs\n",
      "    Workflow Options:\n",
      "      --num-pcs 7 (as int)\n",
      "                        Num. PC to remove from phenotype\n",
      "      --colname-pattern '^[0-9]+'\n",
      "                        column name patter for `grep` in R\n",
      "                        to select phenotype columns eg.\n",
      "                        \"^NA[0-9]+\" to extract sample names\n",
      "  index_vcf:            this step provides VCF file index\n",
      "  SuSiE_2:              Extract cis-SNPs and make fine-\n",
      "                        mapping datasets\n",
      "    Workflow Options:\n",
      "      --max-dist 1000000 (as int)\n",
      "                        Maximum distance to site of\n",
      "                        interest, eg. 1MB up/downstream to\n",
      "                        TSS for gene level QTL\n",
      "      --n-batch 100 (as int)\n",
      "  R_loader:             Data loader utility function\n",
      "  SuSiE_3:              Run finemapping with SuSiE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Frontend communicator is broken. Please restart jupyter server\n"
     ]
    }
   ],
   "source": [
    "!sos run 20180704_MolecularQTL_Workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow can only be executed with magic %run or %sosrun."
     ]
    }
   ],
   "source": [
    "[global]\n",
    "# Specify work directory\n",
    "parameter: cwd = path(\"~/GIT/LargeFiles/finemap_workflow_output\")\n",
    "# X data, the genotype VCF file path\n",
    "parameter: x_data = path()\n",
    "# Y data, the phenotype file paths\n",
    "parameter: y_data = path()\n",
    "# HDF5 file, merged X and Y data\n",
    "parameter: h5_data = path()\n",
    "fail_if(not x_data.is_file() and not h5_data.is_file(), msg = 'Please provide ``--x-data`` or ``--h5-data``!')\n",
    "fail_if(not y_data.is_file() and not h5_data.is_file(), msg = 'Please provide ``--y-data`` or ``--h5-data``!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressing out top PCs on phenotype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may well be better approach to control for covariates etc, but [here](https://github.com/bmvdgeijn/WASP/blob/master/examples/example_data/H3K27ac/get_PCs.R) is workflow from the authors and was deemed sufficient. See their supplemental table of Yang et al 2016 for how many PC to use for each molecular QTLs.\n",
    "\n",
    "Need to cope with missing phenotype data here. See `na.omit` function call in `prcomp` and `na.actions=na.exclude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on phenotype and remove top PCs\n",
    "[SuSiE_1 (Remove top phenotype PC)]\n",
    "# Num. PC to remove from phenotype\n",
    "parameter: num_pcs = 7\n",
    "# column name patter for `grep` in R to select phenotype columns\n",
    "# eg. \"^NA[0-9]+\" to extract sample names\n",
    "parameter: colname_pattern = '^[0-9]+'\n",
    "input: y_data\n",
    "output: f\"{cwd}/{_input:bn}.PC{num_pcs}.removed.gz\"\n",
    "R: expand = \"${ }\", workdir = cwd, stdout = f\"{_output:n}.stdout\"\n",
    "    num_pcs = ${num_pcs}\n",
    "    dat <- read.table(${_input:r}, header=T, comment.char='', check.names=F)\n",
    "    phenotype.matrix <- dat[,5:ncol(dat)]\n",
    "    # extract columns of interest\n",
    "    phenotype.matrix <- phenotype.matrix[,grep(\"${colname_pattern}\", colnames(phenotype.matrix), value = T)]\n",
    "    # perform principal component analysis\n",
    "    pca <- prcomp(na.omit(phenotype.matrix))\n",
    "    # PCA summary\n",
    "    print(summary(pca))\n",
    "    cat(\"output\", num_pcs, \"PCs \\n\")\n",
    "    # remove top PC from phenotype; takes a while\n",
    "    cov_pcs <- pca$rotation[, 1:num_pcs]\n",
    "    new.phenotype.matrix <- do.call(rbind, lapply(1:nrow(phenotype.matrix), function(i) residuals(lm(t(phenotype.matrix[i,]) ~ as.matrix(cov_pcs), na.action=na.exclude))))\n",
    "    colnames(new.phenotype.matrix) <- colnames(phenotype.matrix)\n",
    "    new.dat <- cbind(dat[,1:4], new.phenotype.matrix)\n",
    "    colnames(new.dat)[1] <- 'chr'\n",
    "    write.table(new.dat, gzfile(${_output:r}), sep=\"\\t\", quote=F, col.names=T, row.names=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract per unit variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step provides VCF file index\n",
    "[index_vcf: provides = '{filename}.gz.tbi']\n",
    "depends: executable('tabix')\n",
    "input: f\"{filename}.gz\"\n",
    "bash: expand=True\n",
    "   tabix -p vcf {_input}\n",
    "\n",
    "# Extract cis-SNPs and make fine-mapping datasets\n",
    "[SuSiE_2 (Get per-unit dataset)]\n",
    "depends: Py_Module('pysam'), Py_Module('pandas'), Py_Module('dsc'), Py_Module('rpy2'), f\"{x_data}.tbi\"\n",
    "# Maximum distance to site of interest, eg. 1Mb up/downstream to TSS for gene level QTL\n",
    "parameter: max_dist = 1000000\n",
    "# Number of batches to perform analysis\n",
    "parameter: n_batch = 100\n",
    "batch = [x+1 for x in range(n_batch)]\n",
    "input: for_each = 'batch', concurrent = True\n",
    "output: dynamic(glob.glob('{cwd}/{y_data:bnn}/chr*/*.rds'))\n",
    "python: workdir = cwd, expand = \"${ }\"\n",
    "    def read_header(gzfile):\n",
    "        import gzip\n",
    "        with gzip.open(gzfile,'r') as f:\n",
    "            for line in f:\n",
    "                res = [x.decode() for x in line.split()]\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    def chunk_ranges(items, chunks):\n",
    "        result = []\n",
    "        if items <= chunks:\n",
    "            for i in range(0, items):\n",
    "                result.append((i, i + 1))\n",
    "            return result\n",
    "        chunk_size, extras = divmod(items, chunks)\n",
    "        start = 0\n",
    "        for i in range(0, chunks):\n",
    "            if i < extras:\n",
    "                end = start + chunk_size + 1\n",
    "            else:\n",
    "                end = start + chunk_size\n",
    "\n",
    "            result.append((start, end))\n",
    "            start = end\n",
    "        return result\n",
    "    #\n",
    "    batch = ${_batch}\n",
    "    phenotype_id = read_header(${_input:r})[4:]\n",
    "    vcf_id = read_header(${x_data:r})[9:]\n",
    "    from pathlib import Path\n",
    "    import pysam\n",
    "    tbx = pysam.TabixFile(${x_data:r})    \n",
    "    import pandas as pd, numpy as np\n",
    "    from dsc.dsc_io import save_rds\n",
    "    qts = pd.read_csv(${_input:r}, sep = '\\t')\n",
    "    the_chunk = chunk_ranges(qts.shape[0], ${n_batch})[batch-1]\n",
    "    qts = qts.iloc[the_chunk[0]:the_chunk[1],:]\n",
    "    #\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    for idx, unit in qts.iterrows():\n",
    "        if(i % 100 == 0):\n",
    "            print('[batch %s percent completed] %.2f (%s sec elapsed)' % (batch, (float(i+1)/qts.shape[0])*100, np.around(time.time() - start_time, 2)))\n",
    "        i += 1\n",
    "        chrom = unit['chr']\n",
    "        name = unit['ID'].replace(':', '_')\n",
    "        start = max(unit['start'] - ${max_dist}, 0)\n",
    "        end = unit['start'] + ${max_dist}\n",
    "        phenotypes = np.array(unit.tolist())\n",
    "        genotypes = np.array([row for row in tbx.fetch(chrom, start, end, parser=pysam.asTuple())])\n",
    "        if len(genotypes) == 0:\n",
    "            continue\n",
    "        X_data = pd.DataFrame(genotypes[:,9:].T,\n",
    "                              columns = ['_'.join(x) for x in genotypes[:,[0,1,3,4,2]]], \n",
    "                              index = vcf_id)\n",
    "        Y_data = pd.DataFrame(np.matrix(phenotypes[4:]).T, columns = [name],\n",
    "                              index = phenotype_id).dropna()\n",
    "        merged = Y_data.join(X_data, how='inner').T\n",
    "        Path(f'${cwd}/${y_data:bnn}/{chrom}').mkdir(exist_ok=True, parents=True)\n",
    "        save_rds(merged, f'${cwd}/${y_data:bnn}/{chrom}/{name}.rds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finemapping with SuSiE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader utility function\n",
    "[R_loader: provides = file_target('.sos/data_loader.R')]\n",
    "depends: R_library(\"rhdf5\"), R_library(\"tools\")\n",
    "report: output = f'{_output}'\n",
    "    load_data = function(fn, name) {\n",
    "        library(rhdf5)\n",
    "        dat = h5read(fn, name)\n",
    "        #\n",
    "        X = data.frame(dat$X$block0_values)\n",
    "        colnames(X) = dat$X$axis1\n",
    "        rownames(X) = dat$X$axis0\n",
    "        #\n",
    "        y = data.frame(t(dat$y$block0_values))\n",
    "        colnames(y) = tools::file_path_sans_ext(name)\n",
    "        rownames(y) = dat$y$axis1\n",
    "        y = y[!(rowSums(is.na(y))),,drop=F]\n",
    "        return(merge(y, X, by=0))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run finemapping with SuSiE\n",
    "[SuSiE_3 (SuSiE analysis)]\n",
    "depends: file_target('.sos/data_loader.R')\n",
    "input: group_by = 1, concurrent = True\n",
    "output: f'{_input:n}.SuSiE.complete'\n",
    "R: expand = True, input='.sos/data_loader.R'\n",
    "    saveRDS(1, {_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
