{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Molecular QTL workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "%revisions -s -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data\n",
    "\n",
    "Molecular QTL data from [Yang et al (2016) Science](http://eqtl.uchicago.edu/jointLCL/). Input are genotypes of ~100 YRI samples with their molecular QTL data measured in LCL.\n",
    "\n",
    "- alternative splicing (AS) data is of the primary interest here.\n",
    "\n",
    "### Genotypes\n",
    "\n",
    "[Genotype data for YRI](http://eqtl.uchicago.edu/jointLCL/genotypesYRI.gen.txt.gz) is the conventional VCF format but has dosage for genotypes.\n",
    "\n",
    "### Phenotypes\n",
    "\n",
    "Phenotype data has format:\n",
    "\n",
    "```\n",
    "#Chr\tstart\tend\tID\t18486\t18487\t18488\t18489\t18498\t18499\n",
    "chr1\t880180\t880422\tchr1:880180:880422:clu_15502\t0.201694364955\t0.665990212763\t-1.21881815589\t-0.342480185427\t0.165404160483\t-1.58524292941\n",
    "```\n",
    "\n",
    "The first 4 columns are genomic coordinates info. Others are molecular QTL in samples.\n",
    "\n",
    "We analyze:\n",
    "\n",
    "1. [Alternative splicing](http://eqtl.uchicago.edu/jointLCL/qqnorm_ASintron_RNAseqGeuvadis.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Analysis plan\n",
    "\n",
    "- For each analysis unit (gene, or intron cluster for AS), get the 1MB up/down-stream variants in genotypes\n",
    "- Remove top phenotype PC from phenotype data\n",
    "- Fine-mapping using various methods. SuSiE for starters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "!sos run 20180704_MolecularQTL_Workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Specify work directory\n",
    "parameter: cwd = path(\"~/GIT/LargeFiles/AS_output\")\n",
    "# X data, the genotype VCF file path\n",
    "parameter: x_data = path()\n",
    "# Y data, the phenotype file paths\n",
    "parameter: y_data = path()\n",
    "# HDF5 file, merged X and Y data\n",
    "parameter: h5_data = path()\n",
    "fail_if(not x_data.is_file() and not h5_data.is_file(), msg = 'Please provide ``--x-data`` or ``--h5-data``!')\n",
    "fail_if(not y_data.is_file() and not h5_data.is_file(), msg = 'Please provide ``--y-data`` or ``--h5-data``!')\n",
    "pop = 'YRI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Regressing out top PCs on phenotype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "There may well be better approach to control for covariates etc, but [here](https://github.com/bmvdgeijn/WASP/blob/master/examples/example_data/H3K27ac/get_PCs.R) is workflow from the authors and was deemed sufficient. See their supplemental table of Yang et al 2016 for how many PC to use for each molecular QTLs.\n",
    "\n",
    "Need to cope with missing phenotype data here. See `na.omit` function call in `prcomp` and `na.actions=na.exclude`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# PCA on phenotype and remove top PCs\n",
    "[preprocess_1 (Remove top phenotype PC)]\n",
    "# Num. PC to remove from phenotype\n",
    "parameter: num_pcs = 3 # Table S2 of NIHMS835311-supplement-supplement.pdf\n",
    "# column name patter for `grep` in R to select phenotype columns\n",
    "# eg. \"^NA[0-9]+\" to extract sample names\n",
    "parameter: colname_pattern = '^[0-9]+'\n",
    "input: y_data\n",
    "output: f\"{cwd}/{_input:bn}.PC{num_pcs}.removed.gz\"\n",
    "R: expand = \"${ }\", workdir = cwd, stdout = f\"{_output:n}.stdout\"\n",
    "    num_pcs = ${num_pcs}\n",
    "    dat <- read.table(${_input:r}, header=T, comment.char='', check.names=F)\n",
    "    phenotype.matrix <- dat[,5:ncol(dat)]\n",
    "    # extract columns of interest\n",
    "    phenotype.matrix <- phenotype.matrix[,grep(\"${colname_pattern}\", colnames(phenotype.matrix), value = T)]\n",
    "    # perform principal component analysis\n",
    "    pca <- prcomp(na.omit(phenotype.matrix))\n",
    "    # PCA summary\n",
    "    print(summary(pca))\n",
    "    cat(\"output\", num_pcs, \"PCs \\n\")\n",
    "    # remove top PC from phenotype; takes a while\n",
    "    cov_pcs <- pca$rotation[, 1:num_pcs]\n",
    "    new.phenotype.matrix <- do.call(rbind, lapply(1:nrow(phenotype.matrix), function(i) residuals(lm(t(phenotype.matrix[i,]) ~ as.matrix(cov_pcs), na.action=na.exclude))))\n",
    "    colnames(new.phenotype.matrix) <- colnames(phenotype.matrix)\n",
    "    new.dat <- cbind(dat[,1:4], new.phenotype.matrix)\n",
    "    colnames(new.dat)[1] <- 'chr'\n",
    "    write.table(new.dat, gzfile(${_output:r}), sep=\"\\t\", quote=F, col.names=T, row.names=F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Extract per unit variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# this step provides VCF file index\n",
    "[index_vcf: provides = '{filename}.gz.tbi']\n",
    "depends: executable('tabix')\n",
    "input: f\"{filename}.gz\"\n",
    "bash: expand=True\n",
    "   tabix -p vcf {_input}\n",
    "\n",
    "# Extract cis-SNPs and make fine-mapping datasets\n",
    "[preprocess_2 (Get per-unit dataset)]\n",
    "depends: Py_Module('pysam'), Py_Module('pandas'), Py_Module('feather'), Py_Module('rpy2'), f\"{x_data}.tbi\"\n",
    "# Maximum distance to site of interest, eg. 1Mb up/downstream to TSS for gene level QTL\n",
    "parameter: max_dist = 1000000\n",
    "chroms = [f'chr{x+1}' for x in range(22)]\n",
    "input: for_each = 'chroms', concurrent = True\n",
    "output: dynamic(glob.glob('{cwd}/{y_data:bnn}/chr*/*.rds'))\n",
    "python: workdir = cwd, expand = \"${ }\"\n",
    "    def read_header(gzfile):\n",
    "        import gzip\n",
    "        with gzip.open(gzfile,'r') as f:\n",
    "            for line in f:\n",
    "                res = [x.decode() for x in line.split()]\n",
    "                break\n",
    "        return res\n",
    "\n",
    "    chrom = \"${_chroms}\"\n",
    "    phenotype_id = [f'${pop}_{x}' for x in read_header(${_input:r})[4:]]\n",
    "    vcf_id = [f'${pop}_{x}' for x in read_header(${x_data:r})[9:]]\n",
    "    from pathlib import Path\n",
    "    import pysam\n",
    "    tbx = pysam.TabixFile(${x_data:r})    \n",
    "    import pandas as pd, numpy as np\n",
    "    from feather import write_dataframe\n",
    "    qts = pd.read_csv(${_input:r}, sep = '\\t')\n",
    "    qts = qts.loc[qts['chr'] == chrom]\n",
    "    #\n",
    "    import os, time, tempfile\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    cmds = [\"library(feather)\"]\n",
    "    tf = tempfile.NamedTemporaryFile()\n",
    "    for site in sorted(set(qts['start'].tolist())):\n",
    "        if(i % 100 == 0):\n",
    "            print('[chrom %s percent completed] %.1f (%.1f sec elapsed)' % (chrom, (float(i+1)/qts.shape[0])*100, time.time() - start_time))\n",
    "            if len(cmds) > 1:\n",
    "                with open(tf.name, 'w') as f:\n",
    "                    f.write('\\n'.join(cmds))\n",
    "                os.system(f\"Rscript {tf.name}\")\n",
    "                cmds = [\"library(feather)\"]            \n",
    "        unit = qts.loc[qts['start'] == site]\n",
    "        i += unit.shape[0]\n",
    "        start = max(site - ${max_dist}, 0)\n",
    "        end = site + ${max_dist}\n",
    "        genotypes = np.array([row for row in tbx.fetch(chrom, start, end, parser=pysam.asTuple())])\n",
    "        if len(genotypes) == 0:\n",
    "            continue\n",
    "        Y_data = unit.drop([\"chr\", \"start\", \"end\", \"ID\"], axis=1).T\n",
    "        Y_data.columns = [x.replace(':', '_') for x in unit['ID']]\n",
    "        Y_data.index = phenotype_id\n",
    "        X_data = pd.DataFrame(genotypes[:,9:].T,\n",
    "                              columns = ['_'.join(x) for x in genotypes[:,[2,0,1,3,4]]], \n",
    "                              index = vcf_id)\n",
    "        merged = Y_data.join(X_data, how='inner').astype(np.float32)\n",
    "        Path(f'${cwd}/${y_data:bnn}/{chrom}').mkdir(exist_ok=True, parents=True)\n",
    "        basename = f'${cwd}/${y_data:bnn}/{chrom}/{chrom}_{site}_{max(unit[\"end\"].tolist())}'\n",
    "        write_dataframe(merged, basename + '.feather')\n",
    "        cmds.append(\"saveRDS(read_feather('{0}'), '{1}');system('rm -f {0}')\".format(basename + '.feather', basename + '.rds'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Finemapping with SuSiE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Run finemapping with SuSiE\n",
    "[SuSiE (SuSiE analysis)]\n",
    "depends: R_library('susieR')\n",
    "parameter: maxL = 5\n",
    "parameter: prior_var = 0.1\n",
    "input: glob.glob(f'{cwd}/{y_data:bnn}/chr*/*.rds'), group_by = 1, concurrent = True\n",
    "output: dynamic(glob.glob(f'{cwd}/{y_data:bnn}/SuSiE_CS_*/*.rds'))\n",
    "R: expand = \"${ }\"\n",
    "    library(susieR)\n",
    "    dat = readRDS(${_input:r})\n",
    "    n_y = length(grep(\"^chr\", colnames(dat), value = T))\n",
    "    Y = as.matrix(dat[,1:n_y,drop=F])\n",
    "    X = as.matrix(dat[,(n_y+1):ncol(dat),drop=F])\n",
    "    storage.mode(X) = 'double'\n",
    "    storage.mode(Y) = 'double'\n",
    "    bad = which(sapply(1:ncol(X), function(i) all(is.na(X[,i]))))\n",
    "    snps = colnames(X)[-bad]\n",
    "    X = X[,-bad]\n",
    "    for (r in ncol(Y)) {\n",
    "      keep_rows = which(!is.na(Y[,r]))\n",
    "      x = X[keep_rows,]\n",
    "      y = Y[,r][keep_rows]\n",
    "      z_score = susieR:::calc_z(x,y)\n",
    "      names(z_score) = snps\n",
    "      fitted = susie(x, y,\n",
    "               L=${maxL},\n",
    "               estimate_residual_variance = TRUE, \n",
    "               prior_variance = ${prior_var}, \n",
    "               intercept=FALSE,\n",
    "               tol=1e-3)\n",
    "      sets = susie_get_CS(fitted,\n",
    "                    coverage = 0.95,\n",
    "                    X = x, \n",
    "                    min_abs_corr = 0.4)\n",
    "      pip = susie_get_PIP(fitted, sets$cs_index)\n",
    "      dirname = paste0('${cwd}/${y_data:bnn}/SuSiE_CS_', length(sets$cs_index), '/')\n",
    "      system(paste(\"mkdir -p\", dirname))\n",
    "      saveRDS(list(z_score=z_score,fitted=fitted,sets=sets,pip=pip), paste0(dirname, colnames(Y)[r], '.rds'))\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
