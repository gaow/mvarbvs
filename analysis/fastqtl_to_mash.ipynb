{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eQTL summary statistics formatting\n",
    "\n",
    "This workflow converts `fastqtl` eQTL analysis summary statistics text output to formats more friendly to R analysis. In particular:\n",
    "\n",
    "1. It converts single study results to HDF5 format grouped by genes.\n",
    "2. It combines multiple studies into one single HDF5 file. In the context of GTEx each study is result from one tissue.\n",
    "3. For MASH analysis in particular, it extracts from the complete data a subset of results to compute data driven MASH prior covariance, and to fit the MASH mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "### A list of summary statistics\n",
    "\n",
    "Summary statistics from `fasteqtl`-like output are in text format, one row per gene-snp pair. Columns are:\n",
    "\n",
    "```\n",
    "gene_id \n",
    "variant_id      \n",
    "tss_distance    \n",
    "ma_samples      \n",
    "ma_count        \n",
    "maf     \n",
    "pval_nominal    \n",
    "slope   \n",
    "slope_se\n",
    "```\n",
    "\n",
    "Each analysis (tissue for GTEx) has a separate text file. Additionally there are support files of gene transcription start site coordinates, and SNP coordinates.\n",
    "\n",
    "The workflow takes a list of summary statistics file names, eg, `data/eQTLDataDemo/FastQTLSumStats.list` (can be configured) that has the contents:\n",
    "\n",
    "```\n",
    "Tissue_1.fastqtl.gz\n",
    "Tissue_2.fastqtl.gz\n",
    "...\n",
    "```\n",
    "\n",
    "The first two columns of these files have to be `gene_id` and `variant_id` (column name does not matter). For other contents we only need columns $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value for the summary statistics. In the `fastqtl` output format above it is columns `8, 9, 7`. If your summary statistics file has a different format you can use `--cols` parameter to pass the proper column numbers (current default to `--cols 8 9 7`). **Currently this pipeline only supports and requires summary statistics $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value, not other quantities (eg t statistic)**.\n",
    "\n",
    "### A list of gene names (optional)\n",
    "\n",
    "To speed up merging multiple HDF5 files it helps to provide a list of gene names. Otherwise it takes too much time to figure them out from individual HDF5 files before merger can happen. The gene list has the contents like:\n",
    "\n",
    "```\n",
    "ENSG00000186092.4\n",
    "ENSG00000227232.5\n",
    "ENSG00000228463.9\n",
    "ENSG00000241860.6\n",
    "ENSG00000268903.1\n",
    "ENSG00000269981.1\n",
    "ENSG00000279457.4\n",
    "ENSG00000279928.2\n",
    "...\n",
    "```\n",
    "\n",
    "## Run analysis\n",
    "\n",
    "Under the same folder as this list file, you keep all these listed data files. Then you run:\n",
    "\n",
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb convert \\\n",
    "    --data_list data/eQTLDataDemo/FastQTLSumStats.list \\\n",
    "    --gene_list data/eQTLDataDemo/GTEx_genes.txt\n",
    "```\n",
    "to convert to HDF5 only, and \n",
    "\n",
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb \\\n",
    "    --data_list data/eQTLDataDemo/FastQTLSumStats.list \\\n",
    "    --gene_list data/eQTLDataDemo/GTEx_genes.txt\n",
    "```\n",
    "\n",
    "to convert to HDF5 AND extract MASH input.\n",
    "\n",
    "## More options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run fastqtl_to_mash.ipynb\n",
      "               [workflow_name] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  convert\n",
      "  default\n",
      "  document_it\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd fastqtl_to_mash_output (as path)\n",
      "  --data-list data/eQTL_summary_files.txt (as path)\n",
      "                        A text file containing data-set names\n",
      "  --gene-list . (as path)\n",
      "                        Optionally, a list of gene names.\n",
      "  --msg 'eQTL mapping summary statistics'\n",
      "                        Meta-info tag for HDF5 database\n",
      "  --cols 8 9 7 (as list)\n",
      "                        Columns for betahat, se(betahat) and p-value (1-based\n",
      "                        indexing)\n",
      "  --keep-ensg-version 0 (as int)\n",
      "\n",
      "Sections\n",
      "  convert_0:            Generate utility functions\n",
      "  convert_1:            Convert summary stats gzip format to HDF5\n",
      "  convert_2:            Merge single study data to multivariate data\n",
      "  default_3:            Extract data to fit MASH model\n",
      "    Workflow Options:\n",
      "      --random-per-gene 9 (as int)\n",
      "                        Number of random SNPs to draw per gene for fitting MASH\n",
      "                        mixture\n",
      "  default_4:            Subset and split data, generate Z-score and save to RDS\n",
      "    Workflow Options:\n",
      "      --random-snp-size 20000 (as int)\n",
      "                        Size of mash random SNPs. This specifies the size of\n",
      "                        `random` SNP set and the rest of random SNPs go into\n",
      "                        `random_test` SNP set.\n",
      "      --effects-list NULL (as path)\n",
      "                        A list of effect names (SNP names) to include in mash\n",
      "                        analysis.\n",
      "      --conditions-list NULL (as path)\n",
      "                        A list of condition names (tissue names) to include in\n",
      "                        mash analysis\n",
      "  document_it:          Export workflow to HTML document\n"
     ]
    }
   ],
   "source": [
    "! sos run fastqtl_to_mash.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "\n",
    "If you run the entire workflow, you should find under `./fastqtl_to_mash_output` (can be configured):\n",
    "\n",
    "- Study (tissue) specific HDF5 files of summary statistics\n",
    "- Merged HDF5 from multiple studies\n",
    "- \"*.portable.h5\" data extracted for MASH computations\n",
    "- \"*.mash.rds\" data in RDS format splitted to `strong`/`random` and optionally `random_test`, with Z-scores computed from p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory / output directory\n",
    "parameter: cwd = path('./fastqtl_to_mash_output')\n",
    "# A text file containing data-set names\n",
    "parameter: data_list = path(\"data/eQTL_summary_files.txt\")\n",
    "# Optionally, a list of gene names.\n",
    "parameter: gene_list = path()\n",
    "# Meta-info tag for HDF5 database\n",
    "parameter: msg = \"eQTL mapping summary statistics\"\n",
    "# maximum number of groups per HDF5 file\n",
    "maxsize = 1000\n",
    "# Columns for betahat, se(betahat) and p-value (1-based indexing)\n",
    "parameter: cols = [8, 9, 7]\n",
    "parameter: keep_ensg_version = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline details\n",
    "\n",
    "### HDF5 utilities\n",
    "\n",
    "The HDF5 Python interface `pytables` is used to program the data format conversion workhorse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_0, default_0]\n",
    "# Generate utility functions\n",
    "depends: Py_Module('tables')\n",
    "report: expand = \"${ }\", output = '.sos/utils.py'\n",
    "    import sys, os, re, copy\n",
    "    import numpy as np, pandas as pd, tables as tb\n",
    "    tb.parameters.MAX_GROUP_WIDTH = 51200\n",
    "    # tb.parameters.NODE_CACHE_SLOTS = -51200\n",
    "    # tb.parameters.METADATA_CACHE_SIZE = 1048576 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_SIZE = 2097152 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_NELMTS = 521\n",
    "\n",
    "    class Environment:\n",
    "        def __init__(self):\n",
    "            self.float = np.float32\n",
    "            self.duplicate_tag = '_duplicated_'\n",
    "            self.common_suffix = '.h5'\n",
    "\n",
    "    env = Environment()\n",
    "\n",
    "    class TBData(dict):\n",
    "        def __init__(self, data, name, msg = None, root = '/', complib = 'bzip2'):\n",
    "            '''bzip2 may not be compatible with other hdf5 applications; but zlib is fine'''\n",
    "            self.__root = root.strip('/')\n",
    "            self.__group = name\n",
    "            self.__msg = msg\n",
    "            try:\n",
    "                if type(data) is dict:\n",
    "                    self.update(data)\n",
    "                elif type(data) is str:\n",
    "                    # is file name\n",
    "                    self.__load(tb.open_file(data))\n",
    "                else:\n",
    "                    # is file stream\n",
    "                    self.__load(data)\n",
    "            except tb.exceptions.NoSuchNodeError:\n",
    "                raise ValueError('Cannot find dataset {}!'.format(name))\n",
    "            self.tb_filters = tb.Filters(complevel = 9, complib=complib)\n",
    "\n",
    "        def sink(self, filename):\n",
    "            with tb.open_file(filename, 'a') as f:\n",
    "                if self.__root:\n",
    "                    try:\n",
    "                        f.create_group(\"/\", self.__root)\n",
    "                    except:\n",
    "                        pass\n",
    "                try:\n",
    "                    # there is existing data -- have to merge with current data\n",
    "                    # have to do this because the input file lines are not grouped by gene names!!\n",
    "                    # use try ... except to hopefully faster than if ... else\n",
    "                    # e.g., if not f.__contains__('/{}'.format(self.__group)) ... else ...\n",
    "                    for element in f.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                        if element.name != 'colnames':\n",
    "                            self[element.name] = np.concatenate((element[:], self[element.name]))\n",
    "                except tb.exceptions.NoSuchNodeError:\n",
    "                    f.create_group(\"/\" + self.__root, self.__group,\n",
    "                                   self.__msg if self.__msg else self.__group)\n",
    "                for key in self:\n",
    "                    self.__store_array(key, f)\n",
    "                f.flush()\n",
    "\n",
    "        def dump(self, table, output = False):\n",
    "            if output:\n",
    "                pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames']).to_csv(sys.stdout, na_rep = 'NA')\n",
    "                return None\n",
    "            else:\n",
    "                return pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames'])\n",
    "\n",
    "        def __load(self, fstream):\n",
    "            try:\n",
    "                for element in fstream.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                    self[element.name] = element[:]\n",
    "                fstream.close()\n",
    "            except:\n",
    "                fstream.close()\n",
    "                raise\n",
    "\n",
    "        def __roll_back(self, group, name):\n",
    "            try:\n",
    "                n = getattr(group, name)\n",
    "                n._f_remove()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        def __store_array(self, name, fstream):\n",
    "            if self.__root:\n",
    "                element = getattr(getattr(fstream.root, self.__root), self.__group)\n",
    "            else:\n",
    "                element = getattr(fstream.root, self.__group)\n",
    "            arr = self[name]\n",
    "            if type(arr) is list:\n",
    "                arr = np.array(arr)\n",
    "            self.__roll_back(element, name)\n",
    "            #\n",
    "            if arr.shape != (0,):\n",
    "                ds = fstream.create_carray(element, name, tb.Atom.from_dtype(arr.dtype), arr.shape,\n",
    "                                           filters = self.tb_filters)\n",
    "                ds[:] = arr\n",
    "\n",
    "    def get_tb_grps(filenames, group_name = None):\n",
    "        if isinstance(filenames, str):\n",
    "            filenames = [filenames]\n",
    "        names = set()\n",
    "        for filename in filenames:\n",
    "            with tb.open_file(filename) as f:\n",
    "                names.update([node._v_name for node in (f.root if group_name is None else getattr(f.root, '{}'.format(group_name)))])\n",
    "        return sorted(names)\n",
    "\n",
    "    class SSData:\n",
    "        def __init__(self, header = False):\n",
    "            self.data = {'buffer':{'data':[], 'rownames':[]}, 'output':{}}\n",
    "            self.header = header\n",
    "            self.previous_name = self.current_name = None\n",
    "            self.count = -1\n",
    "\n",
    "        def parse(self, line, ensg_version = 0):\n",
    "            # input line is snp, gene, beta, t, pval\n",
    "            if not line:\n",
    "                self.__reset()\n",
    "                self.current_name = None\n",
    "                return 1\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode()\n",
    "            line = line.strip().split()\n",
    "            self.count += 1\n",
    "            if self.header and self.count == 0:\n",
    "                return 0\n",
    "            #\n",
    "            line[0] = line[0].strip('\"')\n",
    "            if ensg_version == 0:\n",
    "                line[0] = line[0].split('.')[0]\n",
    "            if self.previous_name is None:\n",
    "                self.previous_name = line[0]\n",
    "            self.current_name = line[0]\n",
    "            if self.current_name != self.previous_name:\n",
    "                self.__reset()\n",
    "            self.data['buffer']['data'].append([line[${cols[0]-1}], line[${cols[1]-1}], line[${cols[2]-1}]])\n",
    "            self.data['buffer']['rownames'].append(self.__format_variant_id(line[1]))\n",
    "            return 0\n",
    "        \n",
    "        @staticmethod\n",
    "        def __format_variant_id(value):\n",
    "            value = value.strip('\"').split('_')\n",
    "            if len(value) > 4:\n",
    "                # keep it chr, pos, ref, alt\n",
    "                value = value[:4]\n",
    "            if value[0].startswith('chr'):\n",
    "                value[0] = value[0][3:]\n",
    "            return '_'.join(value)\n",
    "\n",
    "        def __reset(self):\n",
    "            self.data['buffer']['data'] = np.array(self.data['buffer']['data'], dtype = env.float)\n",
    "            self.data['buffer']['rownames'] = np.array(self.data['buffer']['rownames'])\n",
    "            self.data['buffer']['colnames'] = np.array(['beta','se','pval'])\n",
    "            self.data['output'] = copy.deepcopy(self.data['buffer'])\n",
    "            self.data['buffer'] = {'data':[], 'rownames':[]}\n",
    "\n",
    "        def dump(self):\n",
    "            return self.data['output']\n",
    "\n",
    "    class DataMerger(TBData):\n",
    "        def __init__(self, files, name, msg = None):\n",
    "            TBData.__init__(self, {}, name, msg, complib = \"zlib\")\n",
    "            self.files = sorted(files)\n",
    "            self.__group = name\n",
    "\n",
    "        def merge(self):\n",
    "            data = {}\n",
    "            one_snp = None\n",
    "            failure_ct = 0\n",
    "            # Collect data\n",
    "            for item in self.files:\n",
    "                tissue = re.sub(r'{}$'.format(env.common_suffix), '', os.path.basename(item))\n",
    "                try:\n",
    "                    data[tissue] = TBData(item, self.__group)\n",
    "                    if one_snp is None: one_snp = data[tissue]['rownames'][0]\n",
    "                except ValueError:\n",
    "                    data[tissue] = {'data' : np.array([[np.nan, np.nan, np.nan]]), 'rownames': None}\n",
    "                    failure_ct += 1\n",
    "                # Fix row name\n",
    "                # Because in GTEx data file there are duplicated gene-snp pairs having different sumstats!!\n",
    "                if data[tissue]['rownames'] is not None:\n",
    "                    data[tissue]['rownames'] = self.__dedup(data[tissue]['rownames'], item)\n",
    "            if failure_ct == len(self.files):\n",
    "                return 1\n",
    "            # Merge data\n",
    "            for idx, item in enumerate(['beta','se','pval']):\n",
    "                self[item] = pd.concat([pd.DataFrame(\n",
    "                    {tissue : data[tissue]['data'][:,idx]},\n",
    "                    index = data[tissue]['rownames'] if data[tissue]['rownames'] is not None else [one_snp]\n",
    "                    ) for tissue in sorted(data.keys())], axis = 1)\n",
    "                if 'rownames' not in self:\n",
    "                    self['rownames'] = np.array(self[item].index, dtype = str)\n",
    "                if 'colnames' not in self:\n",
    "                    self['colnames'] = np.array(self[item].columns.values.tolist(), dtype = str)\n",
    "                self[item] = np.array(self[item].as_matrix(), dtype = env.float)\n",
    "            # np.savetxt(sys.stdout, self['pval'], fmt='%10.5f')\n",
    "            # print(self['rownames'])\n",
    "            # print(self['colnames'])\n",
    "            return 0\n",
    "\n",
    "        def __dedup(self, seq, filename):\n",
    "            seen = {}\n",
    "            dups = set()\n",
    "            def __is_seen(x, seen):\n",
    "                if x not in seen:\n",
    "                    seen[x] = 0\n",
    "                    return 0\n",
    "                else:\n",
    "                    seen[x] += 1\n",
    "                    dups.add(x)\n",
    "                    return 1\n",
    "            # Tag them\n",
    "            obs = [x if not __is_seen(x, seen) else '%s%s%s' % (x, env.duplicate_tag, seen[x]) for x in seq]\n",
    "            # Log them\n",
    "            if len(dups):\n",
    "                filename = os.path.splitext(filename)[0]\n",
    "                with open(filename + '.error', 'a') as f:\n",
    "                    for item in dups:\n",
    "                        f.write('{}:{} appeared {} times in {}\\n'.\\\n",
    "                                format(self.__group, item, seen[item] + 1, filename))\n",
    "            return obs\n",
    "\n",
    "    def get_gs_pairs(data, name, num = (1, 9), method = 'equal_space'):\n",
    "        '''choose gene-snp pairs from data, controlled by num = (a, b)\n",
    "        for the best gene-snp pair (a = 0 or 1), and b other random \n",
    "        gene-snp pairs'''\n",
    "\n",
    "        def random_sample(x, k):\n",
    "            return sorted(set(np.random.choice(x, min(len(x), k))))\n",
    "\n",
    "        def equal_sample(x, k):\n",
    "            if len(x) < k:\n",
    "                return x\n",
    "            f = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "            return sorted(set([x[i] for i in f(k, len(x))]))\n",
    "\n",
    "        output = {'colnames' : data['colnames']}\n",
    "        lp = data.dump('pval')\n",
    "        lp = lp[np.all(np.isfinite(lp), axis=1)]\n",
    "        #\n",
    "        if lp.empty:\n",
    "            return None\n",
    "        # Find strong SNP-gene pair\n",
    "        lp = -np.log10(lp)\n",
    "        rowidx = np.where(data['rownames'] == lp.max(axis=1).idxmax())[0][0]\n",
    "        if num[0] > 0:\n",
    "            output['strong_rownames'] = ['%s_%s' % (name, data['rownames'][rowidx].decode())]\n",
    "            output['strong'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['strong'][k] = data[k][rowidx, :]\n",
    "        if num[1] > 0:\n",
    "            all_randomidxes = [y for y, x in enumerate(data['rownames']) if x in lp.index and y != rowidx]\n",
    "            sample_randomidxes = randome_sample(all_randomidxes, num[1]) if method == 'random' else equal_sample(all_randomidxes, num[1])\n",
    "            output['random_rownames'] = ['%s_%s' % (name, data['rownames'][x].decode()) for x in sample_randomidxes]\n",
    "            output['random'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['random'][k] = data[k][sample_randomidxes, :]\n",
    "        if not 'strong' in output and not 'random' in output:\n",
    "            output = None\n",
    "        return output\n",
    "\n",
    "    def merge_tmp_h5(output, verbose = 0):\n",
    "        from glob import glob\n",
    "        tmpfiles = list(glob(output + \"_*.tmp\"))\n",
    "        if os.path.isfile(output+'.h5'):\n",
    "            os.remove(output+'.h5')\n",
    "        for item in sorted(tmpfiles):\n",
    "            for name in get_tb_grps(item):\n",
    "                cmd = 'h5copy -i {0} -o {2} -s \"/{1}\" -d \"/{1}\"'.format(item, name, output+'.h5')\n",
    "                if verbose:\n",
    "                    print(cmd)\n",
    "                os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Per study (tissue) conversion\n",
    "\n",
    "For per study conversion I use `bzip2` compression method and `float32` to achieve higher compression rate. This workflow step will generate one HDF5 file per summary statistics file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_1, default_1]\n",
    "# Convert summary stats gzip format to HDF5\n",
    "depends: executable(\"h5copy\")\n",
    "fail_if(not data_list.is_file(), msg = 'Need data list file!')\n",
    "data_files = set(get_output(f\"awk '{{print $1}}' {data_list:e}\").strip().split('\\n'))\n",
    "fail_if(len(data_files) == 0, msg = 'Need input data files!')\n",
    "input: [f'{data_list:d}/{x}' for x in data_files], group_by = 1, concurrent = True\n",
    "output: f'{cwd:a}/{_input:bn}.h5'\n",
    "#task: concurrent = True\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "    \n",
    "python: expand = \"${ }\", input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    import gzip\n",
    "    ssp = SSData(header = True)\n",
    "    group_counts = 0\n",
    "    with gzip.open(${_input:r}) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            quit = ssp.parse(line, ${keep_ensg_version})\n",
    "            if ssp.current_name != ssp.previous_name:\n",
    "                group_counts += 1\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "                    data = TBData(ssp.dump(), ssp.previous_name, \"${msg}\")\n",
    "                    data.sink(\"${_output:n}_%i.tmp\" % (np.ceil(group_counts / ${maxsize})) \n",
    "                                if ${maxsize} > 0 else ${_output:r})\n",
    "                ssp.previous_name = ssp.current_name\n",
    "            if quit:\n",
    "                break\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge per study (tissue) HDF5 to one HDF5\n",
    "\n",
    "This step creates tables for each summary statistic per gene from multiple studies. Rows are effects (SNPs), columns are conditions (tissue names). This time I use `zlib` compression for better compatibility with other HDF5 routines (eg `rhdf5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_2, default_2]\n",
    "# Merge single study data to multivariate data\n",
    "depends: executable(\"h5copy\")\n",
    "output: f'{cwd:a}/{data_list:bn}.h5'\n",
    "#task:\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "\n",
    "python: expand = '${ }', input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    failure_ct = 0\n",
    "    for idx, item in enumerate(gene_names):\n",
    "        ssm = DataMerger([${_input:r,}], item, \"${msg}\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            if ssm.merge() == 0:\n",
    "                ssm.sink(\"${_output:n}_%i.tmp\" % (np.ceil((idx + 1.0) / ${maxsize})) \n",
    "                        if ${maxsize} > 0 else ${_output:r})\n",
    "            else:\n",
    "                failure_ct += 1\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups merged!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "            \n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data for MASH model\n",
    "\n",
    "We need to extract the \"top\" signals based on single tissue analysis to compute MASH priors, as well as some random SNP sets to fit MASH mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_3]\n",
    "# Extract data to fit MASH model\n",
    "\n",
    "# Number of random SNPs to draw per gene for fitting MASH mixture\n",
    "parameter: random_per_gene = 9\n",
    "\n",
    "output: f\"{_input:n}.portable.h5\"\n",
    "#task:\n",
    "\n",
    "python: expand = \"${ }\", input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    output = dict()\n",
    "    output['random'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    output['strong'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    failure_ct = 0\n",
    "    for idx, name in enumerate(gene_names):\n",
    "        # extract the best gene-snp pair or some random gene-snp pairs\n",
    "        res = get_gs_pairs(TBData(${_input:r}, name), name, (1, ${random_per_gene}))\n",
    "        #\n",
    "        if res is None:\n",
    "            failure_ct += 1\n",
    "            continue\n",
    "        for k in output:\n",
    "            if not k in res:\n",
    "                continue\n",
    "            for kk in output[k]:\n",
    "                if kk == 'rownames':\n",
    "                    if output[k]['rownames'] is None:\n",
    "                        output[k]['rownames'] = res[\"{}_rownames\".format(k)]\n",
    "                    else:\n",
    "                        output[k]['rownames'].extend(res[\"{}_rownames\".format(k)])\n",
    "                elif kk == 'colnames':\n",
    "                    if output[k]['colnames'] is None:\n",
    "                        output[k]['colnames'] = res['colnames']\n",
    "                else:\n",
    "                    output[k][kk] = np.vstack((output[k][kk], res[k][kk])) if output[k][kk] is not None else res[k][kk]\n",
    "    #\n",
    "    if failure_ct < len(gene_names):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            for k in output:\n",
    "                TBData(dict(output[k]), k, msg = \"%s, %s gene-snp pair\" % (\"${msg}\", k), complib = 'zlib').sink(${_output:r})\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups extracted!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Z score and save to RDS\n",
    "\n",
    "Here I also implemented two command options to subset data:\n",
    "\n",
    "- `--effects_list`: a list of effect names (row names) to include from output MASH data-set. This option was used in MASH paper to remove SNPs in LD with each other.\n",
    "- `--conditions_list`: a list of conditions (column names) to include in output MASH data-set. Conditions not in this list are excluded from output. This option was used in MASH paper to focus analysis on brain / no-brain tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_4]\n",
    "# Subset and split data, generate Z-score and save to RDS\n",
    "\n",
    "# Size of mash random SNPs. \n",
    "# This specifies the size of `random` SNP set \n",
    "# and the rest of random SNPs go into `random_test` SNP set.\n",
    "parameter: random_snp_size = 20000\n",
    "# A list of effect names (SNP names) to include in mash analysis.\n",
    "parameter: effects_list = path('NULL')\n",
    "# A list of condition names (tissue names) to include in mash analysis\n",
    "parameter: conditions_list = path('NULL')\n",
    "\n",
    "depends: R_library('rhdf5')\n",
    "output: f\"{_input:n}.mash.rds\" if not str(_input).endswith('.portable.h5') else f\"{_input:nn}.mash.rds\"\n",
    "\n",
    "R: expand = \"${ }\"\n",
    "    ConvertP2Z <- function(pval, beta) {\n",
    "      z <- abs(qnorm(pval / 2))\n",
    "      z[which(beta < 0)] <- -1 * z[which(beta < 0)]\n",
    "      return(z)\n",
    "    }\n",
    "\n",
    "    GetSS <- function(table, db) {\n",
    "      dat <- rhdf5::h5read(db, table)\n",
    "      dat$\"z\" <- ConvertP2Z(dat$\"pval\", dat$\"beta\")\n",
    "      for (name in c(\"beta\", \"se\", \"pval\", \"z\")) {\n",
    "        dat[[name]] <- t(dat[[name]])\n",
    "        colnames(dat[[name]]) <- dat$colnames\n",
    "        rownames(dat[[name]]) <- dat$rownames\n",
    "      }\n",
    "      dat$colnames <- dat$rownames <- NULL\n",
    "      return(dat)\n",
    "    }\n",
    "  \n",
    "    SplitTrainTest <- function(dat, table) {\n",
    "        # load data\n",
    "        strong = dat$strong[[table]]\n",
    "        random = dat$random[[table]]\n",
    "        # select rows to keep\n",
    "        num_train = ${random_snp_size}\n",
    "        if (num_train > nrow(random)) {\n",
    "            num_train = nrow(random)\n",
    "        }\n",
    "        train = random[1:num_train,]\n",
    "        if (num_train == nrow(random)) {\n",
    "            test = NULL\n",
    "        } else {\n",
    "            test = random[(num_train+1):nrow(random),]\n",
    "        }\n",
    "        if (${effects_list:r} != 'NULL') {\n",
    "            pout = scan(${effects_list:ar}, what=\"character\", sep=NULL)\n",
    "            strong = strong[(rownames(strong) %in% pout),]\n",
    "            train = train[(rownames(train) %in% pout),]\n",
    "            if (!is.null(test))\n",
    "                test = test[(rownames(test) %in% pout),]                                         \n",
    "        }\n",
    "        if (${conditions_list:r} != 'NULL') {\n",
    "            rout = scan(${conditions_list:ar}, what=\"character\", sep=NULL)\n",
    "            strong = strong[,(colnames(strong) %in% rout),drop=F]\n",
    "            train = train[,(colnames(train) %in% rout),drop=F]\n",
    "            if (!is.null(test))\n",
    "                test = test[,(colnames(test) %in% rout),drop=F]\n",
    "        } \n",
    "\n",
    "        return(list(random = train,\n",
    "               random.test = test,\n",
    "               strong = strong))\n",
    "    }\n",
    "      \n",
    "    SS_data = list(strong = GetSS('strong', ${_input:r}), random = GetSS('random', ${_input:r}))\n",
    "    ztable = SplitTrainTest(SS_data, \"z\")\n",
    "    btable = SplitTrainTest(SS_data, \"beta\")\n",
    "    stable = SplitTrainTest(SS_data, \"se\")\n",
    "    # save output\n",
    "    saveRDS(list(random.z = ztable$random,\n",
    "                 random.test.z = ztable$random.test,\n",
    "                 strong.z = ztable$strong,\n",
    "                 random.b = btable$random,\n",
    "                 random.test.b = btable$random.test,\n",
    "                 strong.b = btable$strong,\n",
    "                 random.s = stable$random,\n",
    "                 random.test.s = stable$random.test,\n",
    "                 strong.s = stable$strong), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export workflow documentation to HTML format\n",
    "\n",
    "```\n",
    "sos run workflow/fastqtl_to_mash document_it\n",
    "```\n",
    "\n",
    "will export the narrative in this workflow notebook to HTML format as a documentation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[document_it]\n",
    "# Export workflow to HTML document\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [f'{cwd:a}/{item:bn}.html' for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output} --template sos-report"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
