{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting tissue specific eQTL summary statistics to HDF5\n",
    "Here I convert GTEx summary statistics to HDF5 format, making it easier to query and share the results. \n",
    "\n",
    "This procedure was initially written and used for GTEx V6 data in 2015. It is now updated to deal with V8 data, for input to `mashr`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "Summar statistics data are in text format, one row per gene-snp pair. Columns are:\n",
    "\n",
    "```\n",
    "gene_id variant_id      tss_distance    ma_samples      ma_count        maf     pval_nominal    slope   slope_se\n",
    "```\n",
    "\n",
    "Each tissue has a separate text file. Additionally there are support files of gene transcription start site coordinates, and SNP coordinates.\n",
    "\n",
    "Since each file has its own collection of genes, to make merger easier, we provide a list of genes extracted from file `GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.gz`. This file has over 56K genes although the eQTL analysis only has the regular > 20K genes. The gene list `GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.annotation` was generated from the `ensembl_annotation` workflow documented in `analysis/20170929_Genotype.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "cwd = '~/Documents/GTExV8'\n",
    "sumstats_files = glob.glob(\"${cwd!a}/GTEx_Analysis_v8_eQTL_all_associations/*.allpairs.txt.gz\")\n",
    "v8db = \"${cwd!a}/GTExV8.ciseQTL.h5\"\n",
    "parameter: msg = \"GTEX V8 fastqtl analysis\"\n",
    "parameter: maxsize = 1000 # maximum number of groups per HDF5 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[convert_0, extract_0]\n",
    "depends: Py_Module('tables')\n",
    "report: output = '.sos/utils.py'\n",
    "import sys, os, re, copy\n",
    "import numpy as np, pandas as pd, tables as tb\n",
    "tb.parameters.MAX_GROUP_WIDTH = 51200\n",
    "# tb.parameters.NODE_CACHE_SLOTS = -51200\n",
    "# tb.parameters.METADATA_CACHE_SIZE = 1048576 * 100000\n",
    "# tb.parameters.CHUNK_CACHE_SIZE = 2097152 * 100000\n",
    "# tb.parameters.CHUNK_CACHE_NELMTS = 521\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.float = np.float64\n",
    "        self.duplicate_tag = '_duplicated_'\n",
    "        self.common_suffix = '.h5'\n",
    "\n",
    "env = Environment()\n",
    "\n",
    "class TBData(dict):\n",
    "    def __init__(self, data, name, msg = None, root = '/', complib = 'bzip2'):\n",
    "        '''bzip2 may not be compatible with other hdf5 applications; but zlib is fine'''\n",
    "        self.__root = root.strip('/')\n",
    "        self.__group = name\n",
    "        self.__msg = msg\n",
    "        try:\n",
    "            if type(data) is dict:\n",
    "                self.update(data)\n",
    "            elif type(data) is str:\n",
    "                # is file name\n",
    "                self.__load(tb.open_file(data))\n",
    "            else:\n",
    "                # is file stream\n",
    "                self.__load(data)\n",
    "        except tb.exceptions.NoSuchNodeError:\n",
    "            raise ValueError('Cannot find dataset {}!'.format(name))\n",
    "        self.tb_filters = tb.Filters(complevel = 9, complib=complib)\n",
    "\n",
    "    def sink(self, filename):\n",
    "        with tb.open_file(filename, 'a') as f:\n",
    "            if self.__root:\n",
    "                try:\n",
    "                    f.create_group(\"/\", self.__root)\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                # there is existing data -- have to merge with current data\n",
    "                # have to do this because the input file lines are not grouped by gene names!!\n",
    "                # use try ... except to hopefully faster than if ... else\n",
    "                # e.g., if not f.__contains__('/{}'.format(self.__group)) ... else ...\n",
    "                for element in f.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                    if element.name != 'colnames':\n",
    "                        self[element.name] = np.concatenate((element[:], self[element.name]))\n",
    "            except tb.exceptions.NoSuchNodeError:\n",
    "                f.create_group(\"/\" + self.__root, self.__group,\n",
    "                               self.__msg if self.__msg else self.__group)\n",
    "            for key in self:\n",
    "                self.__store_array(key, f)\n",
    "            f.flush()\n",
    "\n",
    "    def dump(self, table, output = False):\n",
    "        if output:\n",
    "            pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames']).to_csv(sys.stdout, na_rep = 'NA')\n",
    "            return None\n",
    "        else:\n",
    "            return pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames'])\n",
    "\n",
    "    def __load(self, fstream):\n",
    "        try:\n",
    "            for element in fstream.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                self[element.name] = element[:]\n",
    "            fstream.close()\n",
    "        except:\n",
    "            fstream.close()\n",
    "            raise\n",
    "\n",
    "    def __roll_back(self, group, name):\n",
    "        try:\n",
    "            n = getattr(group, name)\n",
    "            n._f_remove()\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def __store_array(self, name, fstream):\n",
    "        if self.__root:\n",
    "            element = getattr(getattr(fstream.root, self.__root), self.__group)\n",
    "        else:\n",
    "            element = getattr(fstream.root, self.__group)\n",
    "        arr = self[name]\n",
    "        if type(arr) is list:\n",
    "            arr = np.array(arr)\n",
    "        self.__roll_back(element, name)\n",
    "        #\n",
    "        if arr.shape != (0,):\n",
    "            ds = fstream.create_carray(element, name, tb.Atom.from_dtype(arr.dtype), arr.shape,\n",
    "                                       filters = self.tb_filters)\n",
    "            ds[:] = arr\n",
    "\n",
    "def get_tb_grps(filenames, group_name = None):\n",
    "    if isinstance(filenames, str):\n",
    "        filenames = [filenames]\n",
    "    names = set()\n",
    "    for filename in filenames:\n",
    "        with tb.open_file(filename) as f:\n",
    "            names.update([node._v_name for node in (f.root if group_name is None else getattr(f.root, '{}'.format(group_name)))])\n",
    "    return sorted(names)\n",
    "\n",
    "class SSData:\n",
    "    def __init__(self, header = False):\n",
    "        self.data = {'buffer':{'data':[], 'rownames':[]}, 'output':{}}\n",
    "        self.header = header\n",
    "        self.previous_name = self.current_name = None\n",
    "        self.count = -1\n",
    "\n",
    "    def parse(self, line, ensg_version = 0):\n",
    "        # input line is snp, gene, beta, t, pval\n",
    "        if not line:\n",
    "            self.__reset()\n",
    "            self.current_name = None\n",
    "            return 1\n",
    "        line = line.strip().split()\n",
    "        self.count += 1\n",
    "        if self.header and self.count == 0:\n",
    "            return 0\n",
    "        #\n",
    "        if ensg_version == 0:\n",
    "            line[0] = line[0].split('.')[0]\n",
    "        if self.previous_name is None:\n",
    "            self.previous_name = line[0]\n",
    "        self.current_name = line[0]\n",
    "        if self.current_name != self.previous_name:\n",
    "            self.__reset()\n",
    "        self.data['buffer']['data'].append([line[7], line[8], line[6]])\n",
    "        self.data['buffer']['rownames'].append(line[1][3:-4])\n",
    "        return 0\n",
    "\n",
    "    def __reset(self):\n",
    "        self.data['buffer']['data'] = np.array(self.data['buffer']['data'], dtype = env.float)\n",
    "        self.data['buffer']['rownames'] = np.array(self.data['buffer']['rownames'])\n",
    "        self.data['buffer']['colnames'] = np.array(['beta','se','pval'])\n",
    "        self.data['output'] = copy.deepcopy(self.data['buffer'])\n",
    "        self.data['buffer'] = {'data':[], 'rownames':[]}\n",
    "\n",
    "    def dump(self):\n",
    "        return self.data['output']\n",
    "\n",
    "class DataMerger(TBData):\n",
    "    def __init__(self, files, name, msg = None):\n",
    "        TBData.__init__(self, {}, name, msg, complib = \"zlib\")\n",
    "        self.files = sorted(files)\n",
    "        self.__group = name\n",
    "\n",
    "    def merge(self):\n",
    "        data = {}\n",
    "        one_snp = None\n",
    "        failure_ct = 0\n",
    "        # Collect data\n",
    "        for item in self.files:\n",
    "            tissue = re.sub(r'{}$'.format(env.common_suffix), '', os.path.basename(item))\n",
    "            try:\n",
    "                data[tissue] = TBData(item, self.__group)\n",
    "                if one_snp is None: one_snp = data[tissue]['rownames'][0]\n",
    "            except ValueError:\n",
    "                data[tissue] = {'data' : np.array([[np.nan, np.nan, np.nan]]), 'rownames': None}\n",
    "                failure_ct += 1\n",
    "            # Fix row name\n",
    "            # Because in GTEx data file there are duplicated gene-snp pairs having different sumstats!!\n",
    "            if data[tissue]['rownames'] is not None:\n",
    "                data[tissue]['rownames'] = self.__dedup(data[tissue]['rownames'], item)\n",
    "        if failure_ct == len(self.files):\n",
    "            return 1\n",
    "        # Merge data\n",
    "        for idx, item in enumerate(['beta','se','pval']):\n",
    "            self[item] = pd.concat([pd.DataFrame(\n",
    "                {tissue : data[tissue]['data'][:,idx]},\n",
    "                index = data[tissue]['rownames'] if data[tissue]['rownames'] is not None else [one_snp]\n",
    "                ) for tissue in sorted(data.keys())], axis = 1)\n",
    "            if 'rownames' not in self:\n",
    "                self['rownames'] = np.array(self[item].index, dtype = str)\n",
    "            if 'colnames' not in self:\n",
    "                self['colnames'] = np.array(self[item].columns.values.tolist(), dtype = str)\n",
    "            self[item] = np.array(self[item].as_matrix(), dtype = env.float)\n",
    "        # np.savetxt(sys.stdout, self['pval'], fmt='%10.5f')\n",
    "        # print(self['rownames'])\n",
    "        # print(self['colnames'])\n",
    "        return 0\n",
    "\n",
    "    def __dedup(self, seq, filename):\n",
    "        seen = {}\n",
    "        dups = set()\n",
    "        def __is_seen(x, seen):\n",
    "            if x not in seen:\n",
    "                seen[x] = 0\n",
    "                return 0\n",
    "            else:\n",
    "                seen[x] += 1\n",
    "                dups.add(x)\n",
    "                return 1\n",
    "        # Tag them\n",
    "        obs = [x if not __is_seen(x, seen) else '%s%s%s' % (x, env.duplicate_tag, seen[x]) for x in seq]\n",
    "        # Log them\n",
    "        if len(dups):\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            with open(filename + '.error', 'a') as f:\n",
    "                for item in dups:\n",
    "                    f.write('{}:{} appeared {} times in {}\\n'.\\\n",
    "                            format(self.__group, item, seen[item] + 1, filename))\n",
    "        return obs\n",
    "\n",
    "def get_gs_pairs(data, name, num = (1, 9), method = 'equal_space'):\n",
    "    '''choose gene-snp pairs from data, controlled by num = (a, b)\n",
    "    for the best gene-snp pair (a = 0 or 1), and b other random \n",
    "    gene-snp pairs'''\n",
    "    \n",
    "    def random_sample(x, k):\n",
    "        return sorted(set(np.random.choice(x, min(len(x), k))))\n",
    "\n",
    "    def equal_sample(x, k):\n",
    "        f = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "        return sorted(set([x[i] for i in f(k, len(x))]))\n",
    "\n",
    "    output = {'colnames' : data['colnames']}\n",
    "    lp = data.dump('pval')\n",
    "    lp = lp[np.all(np.isfinite(lp), axis=1)]\n",
    "    #\n",
    "    if lp.empty:\n",
    "        return None\n",
    "    # Find max SNP-gene pair\n",
    "    lp = -np.log10(lp)\n",
    "    rowidx = np.where(data['rownames'] == lp.max(axis=1).idxmax())[0][0]\n",
    "    if num[0] > 0:\n",
    "        output['max_rownames'] = ['%s_%s' % (name, data['rownames'][rowidx].decode())]\n",
    "        output['max'] = {}\n",
    "        for k in ['beta', 'pval', 'se']:\n",
    "            output['max'][k] = data[k][rowidx, :]\n",
    "    if num[1] > 0:\n",
    "        all_nullidxes = [y for y, x in enumerate(data['rownames']) if x in lp.index and y != rowidx]\n",
    "        sample_nullidxes = randome_sample(all_nullidxes, num[1]) if method == 'random' else equal_sample(all_nullidxes, num[1])\n",
    "        output['null_rownames'] = ['%s_%s' % (name, data['rownames'][x].decode()) for x in sample_nullidxes]\n",
    "        output['null'] = {}\n",
    "        for k in ['beta', 'pval', 'se']:\n",
    "            output['null'][k] = data[k][sample_nullidxes, :]\n",
    "    if not 'max' in output and not 'null' in output:\n",
    "        output = None\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark from previous runs:\n",
    "\n",
    "| time | original size | HDF5 size | method | data type |\n",
    "|:------:|:-------:|:-------:|:-------:|:-----:|\n",
    "| 118 min| 5G (179,083,485 lines) | 3.8G | TBData, bzip2 | float32|  \n",
    "| 118 min| 5G (179,083,485 lines) | 5.1G | TBData, bzip2 | float64|\n",
    "| 138 min| 5G (179,083,485 lines) | 4.9G | TBData, zlib | float32|\n",
    "| 39 min| 5G (179,083,485 lines) | 6.9G | HPData, zlib | float64|\n",
    "\n",
    "I will use `bzip2` compression and `float32` to make initial compression, then extract for `mashr` input and save to RDS format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Per tissue conversion\n",
    "Converts per tissue summary statistics to HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_1, extract_1]\n",
    "depends: executable(\"h5copy\")\n",
    "input: sumstats_files, group_by = 1, pattern = \"{name}.allpairs.txt.gz\"\n",
    "output: expand_pattern('{_name}.h5')\n",
    "task:\n",
    "python: input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    import gzip\n",
    "    ssp = SSData(header = True)\n",
    "    group_counts = 0\n",
    "    with gzip.open(${_input!r}) as f:\n",
    "        while True:\n",
    "            line = f.readline().decode()\n",
    "            quit = ssp.parse(line)\n",
    "            if ssp.current_name != ssp.previous_name:\n",
    "                group_counts += 1\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category = tb.NaturalNameWarning)\n",
    "                    # warnings.filterwarnings(\"ignore\", category = tb.PerformanceWarning)\n",
    "                    data = TBData(ssp.dump(), ssp.previous_name, ${msg!r})\n",
    "                    data.sink(\"${_output!n}_%i.tmp\" % (np.ceil(group_counts / ${maxsize})) if ${maxsize} > 0 else ${_output!r})\n",
    "                ssp.previous_name = ssp.current_name\n",
    "            if quit:\n",
    "                break\n",
    "    if ${maxsize} > 0:\n",
    "        from glob import glob\n",
    "        tmpfiles = list(glob(\"${_output!n}_*.tmp\"))\n",
    "        for item in sorted(tmpfiles):\n",
    "            for name in get_tb_grps(item):\n",
    "                os.system('h5copy -i {0} -o ${_output} -s \"/{1}\" -d \"/{1}\"'.format(item, name))\n",
    "bash:\n",
    "    rm -f ${_output!n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge per tissue HDF5 to one HDF5\n",
    "Create tables for each summary statistic per gene. Rows are SNPs, columns are tissue names. This time I use `zlib` compression for better compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[convert_2, extract_2]\n",
    "gene_list = get_output(\"cut -f4 -d ' ' ${cwd!a}/GTEx_Analysis_2017-06-05_v8_RNASeQCv1.1.9_gene_reads.gct.annotation\").strip().split()\n",
    "depends: executable(\"h5copy\")\n",
    "output: v8db\n",
    "task:\n",
    "python: input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list!r}:\n",
    "        gene_names = [${gene_list!r,}]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${input!r,}])\n",
    "    failure_ct = 0\n",
    "    for idx, item in enumerate(gene_names):\n",
    "        ssm = DataMerger([${input!r,}], item, ${msg!r})\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.NaturalNameWarning)\n",
    "            if ssm.merge() == 0:\n",
    "                ssm.sink(\"${output!n}_%i.tmp\" % (np.ceil((idx + 1.0) / ${maxsize})) if ${maxsize} > 0 else ${_output!r})\n",
    "            else:\n",
    "                failure_ct += 1\n",
    "    with open(\"${output!n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups merged!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))\n",
    "    if ${maxsize} > 0:\n",
    "        from glob import glob\n",
    "        tmpfiles = list(glob(\"${output!n}_*.tmp\"))\n",
    "        for item in sorted(tmpfiles):\n",
    "            for name in get_tb_grps(item):\n",
    "                os.system('h5copy -i {0} -o ${output} -s \"/{1}\" -d \"/{1}\"'.format(item, name))\n",
    "bash:\n",
    "    rm -f ${_output!n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data for mash analysis\n",
    "We need to extract the max-signals based on single tissue analysis, as well as some null data as training / testing sets.\n",
    "\n",
    "For now we only consider genes having complete data for all tissues. Gene list file here is prepared via:\n",
    "\n",
    "```\n",
    "h5ls GTExV8.ciseQTL.h5 |  awk '{print $1}' > GTExV8.ciseQTL.genes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[extract_3]\n",
    "num_null = 9\n",
    "gene_list = get_output(\"cat ${input!n}.genes\").strip().split()\n",
    "output: \"${input!n}.4MASH.h5\"\n",
    "task:\n",
    "python: input = '.sos/utils.py'\n",
    "    import warnings\n",
    "    if ${gene_list!r}:\n",
    "        gene_names = [${gene_list!r,}]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${input!r,}])\n",
    "    output = {}\n",
    "    output['null'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    output['max'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    failure_ct = 0\n",
    "    for idx, name in enumerate(gene_names):\n",
    "        # extract the best gene-snp pair or some null gene-snp pairs\n",
    "        res = get_gs_pairs(TBData(${input!r}, name), name, (1, ${num_null}))\n",
    "        #\n",
    "        if res is None:\n",
    "            failure_ct += 1\n",
    "            continue\n",
    "        for k in output:\n",
    "            if not k in res:\n",
    "                continue\n",
    "            for kk in output[k]:\n",
    "                if kk == 'rownames':\n",
    "                    if output[k]['rownames'] is None:\n",
    "                        output[k]['rownames'] = res[\"{}_rownames\".format(k)]\n",
    "                    else:\n",
    "                        output[k]['rownames'].extend(res[\"{}_rownames\".format(k)])\n",
    "                elif kk == 'colnames':\n",
    "                    if output[k]['colnames'] is None:\n",
    "                        output[k]['colnames'] = res['colnames']\n",
    "                else:\n",
    "                    output[k][kk] = np.vstack((output[k][kk], res[k][kk])) if output[k][kk] is not None else res[k][kk]\n",
    "    #\n",
    "    if failure_ct < len(gene_names):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.NaturalNameWarning)\n",
    "            for k in output:\n",
    "                TBData(dict(output[k]), k, msg = \"%s, %s gene-snp pair\" % (${msg!r}, k), complib = 'zlib').sink(${output!r})\n",
    "    with open(\"${output!n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups extracted!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))\n",
    "\n",
    "[extract_4]\n",
    "# convert HDF5 to RDS\n",
    "depends: R_library('rhdf5')\n",
    "output: \"${input!n}.rds\"\n",
    "task:\n",
    "R:\n",
    "ConvertP2Z <- function(pval, beta) {\n",
    "  z <- abs(qnorm(pval / 2))\n",
    "  z[which(beta < 0)] <- -1 * z[which(beta < 0)]\n",
    "  return(z)\n",
    "}\n",
    "\n",
    "GetSS <- function(table, db) {\n",
    "  dat <- rhdf5::h5read(db, table)\n",
    "  dat$\"z\" <- ConvertP2Z(dat$\"pval\", dat$\"beta\")\n",
    "  for (name in c(\"beta\", \"se\", \"pval\", \"z\")) {\n",
    "    dat[[name]] <- t(dat[[name]])\n",
    "    colnames(dat[[name]]) <- dat$colnames\n",
    "    rownames(dat[[name]]) <- dat$rownames\n",
    "  }\n",
    "  dat$colnames <- dat$rownames <- NULL\n",
    "  return(dat)\n",
    "}\n",
    "\n",
    "saveRDS(list(max = GetSS('max', ${input!r}), null = GetSS('null', ${input!r})), ${output!r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run workflow\n",
    "To convert to HDF5 only\n",
    "```\n",
    "sos run analysis/20170926_eQTLSummary_HDF5.ipynb convert\n",
    "```\n",
    "To convert to HDF5 AND extract `mash` input\n",
    "```\n",
    "sos run analysis/20170926_eQTLSummary_HDF5.ipynb extract\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For V8 data, 39784 genes are found having data in at least one of the 49 tissues. Out of them, 15632 have non-missing data in all tissues and thus extracted. The resulting HDF5 file is 153M (converted also to RDS file which is 211M)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
