{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype of VEM in M&M ASH model\n",
    "This is the VEM step of M&M ASH model, version 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M&M ASH model\n",
    "\n",
    "We assume the following multivariate, multiple regression model with $N$ samples, $J$ effects and $R$ conditions (and **without covariates, for the time being**)\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N\\times R} = \\bs{X}_{N \\times J}\\bs{B}_{J \\times R} + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Lambda}^{-1}),\\\\\n",
    "\\bs{\\Lambda} &= \\diag(\\lambda_1,\\ldots,\\lambda_R).\n",
    "\\end{align}\n",
    "\n",
    "We assume true effects $\\bs{b}_j$ (rows of $\\bs{B}$) are iid with prior distribution of mixtures of multivariate normals\n",
    "\n",
    "$$p(\\bs{b}_j) = \\sum_{t = 0}^T\\pi_t\\N_R(\\bs{b}_j | \\bs{0}, \\bs{V}_t),$$\n",
    "\n",
    "Where the $\\bs{V}_t$'s are $R \\times R$ covariance matrices and the $\\pi_t$'s are their weights.\n",
    "\n",
    "We place Gamma prior on $\\bs{\\Lambda}$\n",
    "\n",
    "$$\\lambda_r \\overset{iid}{\\sim} \\gm(\\alpha, \\beta),$$\n",
    "\n",
    "and set $\\alpha = \\beta = 0$ so that it is equivalent to estimating $\\bs{\\Lambda}$ via maximum likelihood.\n",
    "\n",
    "We can augment the prior of $\\bs{b}_j$ by indicator vector $\\bs{\\gamma}_j \\in \\mathbb{R}^T$ for membership of $\\bs{b}_j$ into one of the $T$ mixture groups. The densities involved are\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{Y},\\bs{B},\\bs{\\Gamma},\\bs{\\Lambda}) &= p(\\bs{Y}|\\bs{B}, \\bs{\\Lambda})p(\\bs{B}|\\bs{\\Gamma})p(\\bs{\\Gamma})p(\\bs{\\Lambda}), \\\\\n",
    "p(\\bs{Y}|\\bs{B}, \\bs{\\Lambda}) &= N_{N \\times R}(\\bs{X}\\bs{B}, \\bs{I}_N, \\bs{\\Lambda}^{-1}), \\\\\n",
    "p(\\lambda_r|\\alpha,\\beta) &= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}\\lambda_r^{\\alpha - 1}\\exp\\{-\\beta\\lambda_r\\}, \\\\\n",
    "p(\\bs{b}_j|\\bs{\\gamma}_j) &= \\prod_{t = 0}^T\\left[\\N(\\bs{b}_j|\\bs{0},\\bs{V}_t)\\right]^{\\gamma_{jt}},\\\\\n",
    "p(\\bs{\\gamma}_j) &= \\prod_{t = 0}^{T} \\pi_t^{\\gamma_{jt}}.\n",
    "\\end{align}\n",
    "\n",
    "**We assume $V_t$'s and their corresponding $\\pi_t$'s are known. In practice we use `mashr` to estimate these quantities and provide them to M&M.**\n",
    "\n",
    "### Variational approximation to densities\n",
    "\n",
    "For the posterior of $\\bs{B}$ we seek an independent variational approximation based on\n",
    "\n",
    "\\begin{align}\n",
    "q(\\bs{B}, \\bs{\\Gamma}, \\bs{\\Lambda}) = q(\\bs{\\Lambda})\\prod_{j = 1}^{J}q(\\bs{b}_j,\\bs{\\gamma}_j),\n",
    "\\end{align}\n",
    "\n",
    "so that we can maximize over $q$ the following lower bound of the marginal log-likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\bs{Y}) \\geq \\mathcal{L}(q) = \\int q(\\bs{B}, \\bs{\\Gamma}, \\bs{\\Lambda}) \\log\\left\\{\\frac{p(\\bs{Y},\\bs{B},\\bs{\\Gamma},\\bs{\\Lambda})}{q(\\bs{B}, \\bs{\\Gamma}, \\bs{\\Lambda})}\\right\\}\\dif\\bs{B}\\dif\\bs{\\Gamma}\\dif\\bs{\\Lambda},\n",
    "\\end{align}\n",
    "\n",
    "Gao & Wei have previously developed [a version that assumes $\\Lambda = I_R$](https://github.com/gaow/mvarbvs/blob/master/writeup/identity_cov/mnmash.pdf). This version generalized it to a diagonal matrix with Gamma priors. [David has developed a version](https://www.overleaf.com/11985539jvwgjhrqnrry#/45465793/) that assumes a diagonal plus low rank structure -- the model of that version is a bit different from shown here, and will be prototyped later after this version works.\n",
    "\n",
    "### Core updates\n",
    "\n",
    "The complete derivation of updates are documented elsewhere (in the two PDF write-ups whose links are shown above). Here I document core updates to guide implementation of the algorithm.\n",
    "\n",
    "Let $E[\\bs{R}_{-j}] := \\bs{Y} - \\bs{X}\\bs{\\mu}_{\\bs{B}} + \\bs{x}_j\\bs{\\mu}_{\\bs{B}[j, ]}^{\\intercal}$, then\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{\\xi}_j &= E\\left[\\bs{R}_{-j}\\right]^{\\intercal}\\bs{x}_j\\|\\bs{x}_j\\|^{-2}, \\\\\n",
    "\\bs{\\Sigma}_{jt} &= \\left(\\bs{V}_t^{-1} + \\|\\bs{x}_j\\|^2\\bs{\\Lambda}\\right)^{-1}, \\\\\n",
    "\\bs{\\mu}_{jt} &= \\bs{\\Sigma}_{jt}\\bs{\\Lambda}E\\left[\\bs{R}_{-k}\\right]^{\\intercal}\\bs{x}_j, \\\\\n",
    "w_{jt} &= \\frac{\\pi_t\\N(\\bs{\\xi}_j|\\bs{0}, \\bs{V}_t + \\bs{\\Lambda}^{-1}\\|\\bs{x}_j\\|^{-2})}{\\sum_{t = 0}^T\\pi_t\\N(\\bs{\\xi}_j|\\bs{0}, \\bs{V}_t + \\bs{\\Lambda}^{-1}\\|\\bs{x}_j\\|^{-2})},\\\\\n",
    "\\bs{\\mu}_{\\bs{B}[j, ]}  &= \\sum_{t = 0}^T w_{jt}\\bs{\\mu}_{jt}\n",
    "\\end{align}\n",
    "\n",
    "We update until the lower bound $\\mathcal{L}(q)$ converges.\n",
    "\n",
    "## Initialization\n",
    "\n",
    "* We fit `mashr` with effects learned from univariate analysis to obtain $\\pi_t$ and $V_t$\n",
    "  * For the first round the effects are \"LD-polluted\"\n",
    "* Use multivariate LASSO to get the ordering of $X$ for input. Similar approach has been previousely used with `varbvs`.\n",
    "* We \"stack\" expression data under multiple conditions and impute missing data with mean imputation or `softImpute` for a completed $Y$ matrix.\n",
    "  * This version of the model does not impute missing data in $Y$ in its variational updates although this will be added in next version.\n",
    "* We regress out covariates beforehand\n",
    "  * Same approach taken by Guan & Stephens 2011 yet not Carbonetto & Stephens 2012\n",
    "  * In next version we will preprocess covariates by \"stacking\" them together and perform a low-rank decomposition / imputation. For example for 50 tissues there will be a blocked matrix with a total of over 1000 PEER factors when stacked together, with non-random missing data. We will perform a low rank approximation to hopefully only keep < 50 PEER. We will then control for covariates in the M&M model.\n",
    "  \n",
    "**In this notebook we use a test data set of 2 tissues: Thyroid and Lung. As a first pass we also fix $\\bs{V}$ as a null matrix plus an identity matrix, with weights and $\\pi_0=0.9$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "R",
    "scrolled": true,
    "tags": [
     "report_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of 2\n",
      " $ Y:'data.frame':\t698 obs. of  2 variables:\n",
      "  ..$ Thyroid: num [1:698] 0.163 0.436 -0.212 0.327 -0.698 ...\n",
      "  ..$ Lung   : num [1:698] 0.77011 0.77799 -0.65361 0.00672 -0.36792 ...\n",
      " $ X: num [1:698, 1:7492] 1 0 0 0 0 1 1 0 1 1 ...\n",
      "  ..- attr(*, \"dimnames\")=List of 2\n",
      "  .. ..$ : chr [1:698] \"GTEX-111CU\" \"GTEX-111FC\" \"GTEX-111VG\" \"GTEX-111YS\" ...\n",
      "  .. ..$ : chr [1:7492] \"chr1_170185243_G_A_b38\" \"chr1_170185272_T_C_b38\" \"chr1_170185405_C_A_b38\" \"chr1_170185417_G_A_b38\" ...\n",
      " num [1:7492, 1:2] -0.0341 -0.07 -0.0492 -0.07 -0.07 ...\n",
      " num [1:7492, 1:2] 0.0393 0.0706 0.0555 0.0706 0.0706 ...\n"
     ]
    }
   ],
   "source": [
    "dat = readRDS('/home/gaow/Documents/GTExV8/Thyroid.Lung.FMO2.filled.rds')\n",
    "str(dat)\n",
    "attach(dat)\n",
    "univariate_regression = function(X, y, Z = NULL){\n",
    "    if (!is.null(Z)) {\n",
    "        y = .lm.fit(Z, y)$residuals\n",
    "    }\n",
    "    calc_stderr = function(X, residuals) { sqrt(diag(sum(residuals^2) / (nrow(X) - 2) * chol2inv(chol(t(X) %*% X)))) }\n",
    "    output = do.call(rbind, \n",
    "                  lapply(c(1:ncol(X)), function(i) { \n",
    "                      g = .lm.fit(cbind(1, X[,i]), y)\n",
    "                      return(c(coef(g)[2], calc_stderr(cbind(1, X[,i]), g$residuals)[2]))\n",
    "                  })\n",
    "                 )\n",
    "    return(list(betahat = output[,1], sebetahat = output[,2], \n",
    "                new_y = y))\n",
    "}\n",
    "\n",
    "betahat = matrix(0, dim(X)[2], dim(Y)[2])\n",
    "sebetahat = matrix(0, dim(X)[2], dim(Y)[2])\n",
    "for (i in 1:dim(Y)[2]) {\n",
    "    res = univariate_regression(X, Y[,i])\n",
    "    betahat[,i] = res$betahat\n",
    "    sebetahat[,i] = res$sebetahat\n",
    "}\n",
    "\n",
    "str(betahat)\n",
    "str(sebetahat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: feather\n"
     ]
    }
   ],
   "source": [
    "%get X Y betahat sebetahat --from R\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  1.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "Y = Y.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "Python3",
    "tags": [
     "report_output"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16348104,  0.77010917],\n",
       "       [ 0.43588995,  0.77798736],\n",
       "       [-0.21237311, -0.65361193],\n",
       "       ..., \n",
       "       [ 0.62036618, -0.0035004 ],\n",
       "       [ 0.00279156, -0.05439095],\n",
       "       [-0.14650835,  0.29935286]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: mash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">Cell content saved to <a href=\"/home/gaow/GIT/software/libgaow/py/src/model_mash.py\" target=\"_blank\">/home/gaow/GIT/software/libgaow/py/src/model_mash.py</a></div>"
      ],
      "text/plain": [
       "Cell content saved to /home/gaow/GIT/software/libgaow/py/src/model_mash.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%save /home/gaow/GIT/software/libgaow/py/src/model_mash.py -f\n",
    "#!/usr/bin/env python3\n",
    "__author__ = \"Gao Wang\"\n",
    "__copyright__ = \"Copyright 2016, Stephens lab\"\n",
    "__email__ = \"gaow@uchicago.edu\"\n",
    "__license__ = \"MIT\"\n",
    "__version__ = \"0.1.0\"\n",
    "\n",
    "import numpy as np, scipy as sp\n",
    "from scipy.stats import norm, multivariate_normal as mvnorm\n",
    "from collections import OrderedDict\n",
    "\n",
    "def inv_sympd(m):\n",
    "    '''\n",
    "    Inverse of symmetric positive definite\n",
    "    https://stackoverflow.com/questions/40703042/more-efficient-way-to-invert-a-matrix-knowing-it-is-symmetric-and-positive-semi\n",
    "    '''\n",
    "    zz , _ = sp.linalg.lapack.dpotrf(m, False, False)\n",
    "    inv_m , info = sp.linalg.lapack.dpotri(zz)\n",
    "    # lapack only returns the upper or lower triangular part\n",
    "    return np.triu(inv_m) + np.triu(inv_m, k=1).T\n",
    "\n",
    "def get_svs(s, V):\n",
    "    '''\n",
    "    diag(s) @ V @ diag(s)\n",
    "    '''\n",
    "    return (s * V.T).T * s\n",
    "\n",
    "class LikelihoodMASH:\n",
    "    def __init__(self, data):\n",
    "        self.J = data.B.shape[0]\n",
    "        self.R = data.B.shape[1]\n",
    "        self.P = len(data.U)\n",
    "        self.data = data\n",
    "        self.data.lik = {'relative_likelihood' : None,\n",
    "                         'lfactor': None,\n",
    "                         'marginal_loglik': None,\n",
    "                         'loglik': None,\n",
    "                         'null_loglik': None,\n",
    "                         'alt_loglik': None}\n",
    "        self.debug = None\n",
    "\n",
    "    def compute_log10bf(self):\n",
    "        self.data.log10bf = (self.data.lik['alt_loglik'] -  self.data.lik['null_loglik']) / np.log(10)\n",
    "\n",
    "    def compute_relative_likelihood_matrix(self):\n",
    "        matrix_llik = self._calc_likelihood_matrix_comcov() if self.data.is_common_cov() \\\n",
    "                      else self._calc_likelihood_matrix()\n",
    "        lfactors = np.vstack(np.amax(matrix_llik, axis = 1))\n",
    "        self.data.lik['relative_likelihood'] = np.exp(matrix_llik - lfactors)\n",
    "        self.data.lik['lfactor'] = lfactors\n",
    "\n",
    "    def _calc_likelihood_matrix(self):\n",
    "        loglik = np.zeros((self.J, self.P))\n",
    "        for j in range(self.J):\n",
    "            sigma_mat = get_svs(self.data.S[j,:], self.data.V)\n",
    "            try:\n",
    "                loglik[j,:] = np.array([mvnorm.logpdf(self.data.B[j,:], cov = sigma_mat + self.data.U[p], allow_singular = True) for p in self.data.U])\n",
    "            except Exception:\n",
    "                self.debug = {'j': j, 'covs': [sigma_mat + self.data.U[p] for p in self.data.U]}\n",
    "                raise\n",
    "        return loglik\n",
    "\n",
    "    def _calc_likelihood_matrix_comcov(self):\n",
    "        sigma_mat = get_svs(self.data.S[0,:], self.data.V)\n",
    "        return np.array([mvnorm.logpdf(self.data.B, cov = sigma_mat + self.data.U[p]) for p in self.data.U])\n",
    "\n",
    "    def compute_loglik_from_matrix(self, options = ['all', 'alt', 'null']):\n",
    "        '''\n",
    "        data.lik.relative_likelihood first column is null, the rest are alt\n",
    "        '''\n",
    "        if 'marginal' in options:\n",
    "            self.data.lik['marginal_loglik'] = np.log(self.data.lik['relative_likelihood'] @ self.data.pi) + self.data.lik['lfactor'] - np.sum(np.log(self.data.S), axis = 0)\n",
    "            self.data.lik['loglik'] = np.sum(self.data.lik['marginal_loglik'])\n",
    "        if 'alt' in options:\n",
    "            self.data.lik['alt_loglik'] = np.log(self.data.lik['relative_likelihood'][:,1:] @ (self.data.pi[1:] / (1 - self.data.pi[0]))) + self.data.lik['lfactor'] - np.sum(np.log(self.data.S), axis = 1)\n",
    "        if 'null' in options:\n",
    "            self.data.lik['null_loglik'] = np.log(self.data.lik['relative_likelihood'][:,0]) + self.data.lik['lfactor'] - np.sum(np.log(self.data.S), axis = 1)\n",
    "\n",
    "class PosteriorMASH:\n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        // @param b_mat J by R\n",
    "        // @param s_mat J by R\n",
    "        // @param v_mat R by R\n",
    "        // @param U_cube list of prior covariance matrices, for each mixture component P by R by R\n",
    "        '''\n",
    "        self.J = data.B.shape[0]\n",
    "        self.R = data.B.shape[1]\n",
    "        self.P = len(data.U)\n",
    "        self.data = data\n",
    "        self.data.post_mean_mat = np.zeros((self.R, self.J))\n",
    "        self.data.post_mean2_mat = np.zeros((self.R, self.J))\n",
    "        self.data.neg_prob_mat = np.zeros((self.R, self.J))\n",
    "        self.data.zero_prob_mat = np.zeros((self.R, self.J))\n",
    "\n",
    "    def compute_posterior_weights(self):\n",
    "        d = (self.data.pi * self.data.lik['relative_likelihood'])\n",
    "        self.data.posterior_weights = (d.T / np.sum(d, axis = 1))\n",
    "\n",
    "    def compute_posterior(self):\n",
    "        for j in range(self.J):\n",
    "            Vinv_mat = inv_sympd(get_svs(self.data.S[j,:], self.data.V))\n",
    "            mu1_mat = np.zeros((self.R, self.P))\n",
    "            mu2_mat = np.zeros((self.R, self.P))\n",
    "            zero_mat = np.zeros((self.R, self.P))\n",
    "            neg_mat = np.zeros((self.R, self.P))\n",
    "            for p, name in enumerate(self.data.U.keys()):\n",
    "                U1_mat = self.get_posterior_cov(Vinv_mat, self.data.U[name])\n",
    "                mu1_mat[:,p] = self.get_posterior_mean_vec(self.data.B[j,:], Vinv_mat, U1_mat)\n",
    "                sigma_vec = np.sqrt(np.diag(U1_mat))\n",
    "                null_cond = (sigma_vec == 0)\n",
    "                mu2_mat[:,p] = np.square(mu1_mat[:,p]) + np.diag(U1_mat)\n",
    "                if not null_cond.all():\n",
    "                    neg_mat[np.invert(null_cond),p] = norm.cdf(mu1_mat[np.invert(null_cond),p], scale=sigma_vec[np.invert(null_cond)])\n",
    "                zero_mat[null_cond,p] = 1.0\n",
    "            self.data.post_mean_mat[:,j] = mu1_mat @ self.data.posterior_weights[:,j]\n",
    "            self.data.post_mean2_mat[:,j] = mu2_mat @ self.data.posterior_weights[:,j]\n",
    "            self.data.neg_prob_mat[:,j] = neg_mat @ self.data.posterior_weights[:,j]\n",
    "            self.data.zero_prob_mat[:,j] = zero_mat @ self.data.posterior_weights[:,j]\n",
    "\n",
    "    def compute_posterior_comcov(self):\n",
    "        Vinv_mat = inv_sympd(get_svs(self.data.S[0,:], self.data.V))\n",
    "        for p, name in enumerate(self.data.U.keys()):\n",
    "            zero_mat = np.zeros((self.R, self.P))\n",
    "            U1_mat = self.get_posterior_cov(Vinv_mat, self.data.U[name])\n",
    "            mu1_mat = self.get_posterior_mean_mat(self.data.B, Vinv_mat, U1_mat)\n",
    "            sigma_vec = np.sqrt(np.diag(U1_mat))\n",
    "            null_cond = (sigma_vec == 0)\n",
    "            sigma_mat = np.repeat(sigma_vec, self.J, axis = 1)\n",
    "            neg_mat = np.zeros((self.R, self.J))\n",
    "            if not null_cond.all():\n",
    "                neg_mat[np.invert(null_cond),:] = norm.cdf(mu1_mat[np.invert(null_cond),:], scale = sigma_mat[np.invert(null_cond),:])\n",
    "            m2_mat = np.square(mu1_mat) + np.diag(U1_mat)\n",
    "            zero_mat[null_cond,:] = 1.0\n",
    "            self.data.post_mean_mat += posterior_weights[p,:] * mu1_mat\n",
    "            self.data.post_mean2_mat += posterior_weights[p,:] * mu2_mat\n",
    "            self.data.neg_prob_mat += posterior_weights[p,:] * neg_mat\n",
    "            self.data.zero_prob_mat += posterior_weights[p,:] * zero_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def get_posterior_mean_vec(B, V_inv, U):\n",
    "        return U @ (V_inv @ B)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_posterior_mean_mat(B, V_inv, U):\n",
    "        return B @ V_inv @ U\n",
    "\n",
    "    @staticmethod\n",
    "    def get_posterior_cov(V_inv, U):\n",
    "        return U @ inv_sympd(V_inv @ U + np.identity(U.shape[0]))\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, data):\n",
    "        obj = cls(data)\n",
    "        obj.compute_posterior_weights()\n",
    "        if data.is_common_cov():\n",
    "            obj.compute_posterior_comcov()\n",
    "        else:\n",
    "            obj.compute_posterior()\n",
    "\n",
    "class PriorMASH:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.R = data.B.shape[1]\n",
    "\n",
    "    def expand_cov(self, use_pointmass = True):\n",
    "        def product(x,y):\n",
    "            for item in y:\n",
    "                yield x*item\n",
    "        res = OrderedDict()\n",
    "        if use_pointmass:\n",
    "            res['null'] = np.zeros((self.R, self.R))\n",
    "        res.update(OrderedDict(sum([[(f\"{p}.{i+1}\", g) for i, g in enumerate(product(self.data.U[p], np.square(self.data.grid)))] for p in self.data.U], [])))\n",
    "        self.data.U = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility: regression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"sos_hint\">Cell content saved to <a href=\"/home/gaow/GIT/software/libgaow/py/src/regression_data.py\" target=\"_blank\">/home/gaow/GIT/software/libgaow/py/src/regression_data.py</a></div>"
      ],
      "text/plain": [
       "Cell content saved to /home/gaow/GIT/software/libgaow/py/src/regression_data.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%save /home/gaow/GIT/software/libgaow/py/src/regression_data.py -f\n",
    "#!/usr/bin/env python3\n",
    "__author__ = \"Gao Wang\"\n",
    "__copyright__ = \"Copyright 2016, Stephens lab\"\n",
    "__email__ = \"gaow@uchicago.edu\"\n",
    "__license__ = \"MIT\"\n",
    "__version__ = \"0.1.0\"\n",
    "\n",
    "#from model_mash import PriorMASH, LikelihoodMASH, PosteriorMASH\n",
    "\n",
    "class RegressionData:\n",
    "    def __init__(self, X = None, Y = None, Z = None, B = None, S = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.B = B\n",
    "        self.S = S\n",
    "        self.lik = None\n",
    "        self.l10bf = None\n",
    "\n",
    "    def set_prior(self):\n",
    "        pass\n",
    "\n",
    "    def calc_likelihood(self):\n",
    "        pass\n",
    "\n",
    "    def calc_posterior(self):\n",
    "        pass\n",
    "\n",
    "    def calc_bf(self):\n",
    "        pass\n",
    "\n",
    "class MASHData(RegressionData):\n",
    "    def __init__(self, X = None, Y = None, Z = None, B = None, S = None):\n",
    "        RegressionData.__init__(self, X, Y, Z, B, S)\n",
    "        self.post_mean_mat = None\n",
    "        self.post_mean2_mat = None\n",
    "        self.neg_prob_mat = None\n",
    "        self.zero_prob_mat = None\n",
    "        self._is_common_cov = None\n",
    "        self.V = None\n",
    "        self.U = None\n",
    "        self.pi = None\n",
    "        self.posterior_weights = None\n",
    "        self.grid = None\n",
    "\n",
    "    def is_common_cov(self):\n",
    "        if self._is_common_cov is None and self.S is not None:\n",
    "            self._is_common_cov = (self.S.T == self.S.T[0,:]).all()\n",
    "        return self._is_common_cov\n",
    "\n",
    "    def calc_posterior(self):\n",
    "        PosteriorMASH.apply(self)\n",
    "\n",
    "    def calc_likelihood(self):\n",
    "        LikelihoodMASH.apply(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "data = MASHData(X = X, Y = Y, B = betahat, S = sebetahat)\n",
    "data.U = {'identity': np.identity(2)}\n",
    "data.V = np.identity(2)\n",
    "data.pi = np.array([0.9, 0.05, 0.05])\n",
    "data.grid = [0.5, 1]\n",
    "prior = PriorMASH(data)\n",
    "prior.expand_cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "lik = LikelihoodMASH(data)\n",
    "lik.compute_relative_likelihood_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "lik.compute_loglik_from_matrix(options = ['alt', 'null'])\n",
    "lik.compute_log10bf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "PosteriorMASH.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "### multivariate normal issue\n",
    "FIXME: should be infinity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mvnorm.logpdf(np.array([0,0]), cov = np.array([[0,0],[0,0]]), allow_singular = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VEM updates\n",
    "The Core function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "def singe_snp_multivariate(data, Y):\n",
    "    '''\n",
    "    single snp bayes regression of Y on each column of X\n",
    "    under MASH model\n",
    "    Y is N by R matrix\n",
    "    X is N by J matrix\n",
    "    Assume residual variance is identity for now\n",
    "    '''\n",
    "    data.reset(Y=Y)\n",
    "    lik = LikelihoodMASH(data)\n",
    "    lik.compute_relative_likelihood_matrix()\n",
    "    lik.compute_loglik_from_matrix(options = ['alt', 'null'])\n",
    "    lik.compute_log10bf()\n",
    "    PosteriorMASH.apply(data)\n",
    "    return {'alpha': data.log10bf / np.sum(data.log10bf), 'mu': data.post_mean_mat, 's2': data.post_mean2_mat - np.square(data.post_mean_mat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "Python3",
   "kernels": [
    [
     "Python3",
     "python3",
     "Python3",
     "#FFE771"
    ],
    [
     "R",
     "ir",
     "R",
     "#DCDCDA"
    ],
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "panel": {
    "displayed": false,
    "height": "655px",
    "style": "side"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
