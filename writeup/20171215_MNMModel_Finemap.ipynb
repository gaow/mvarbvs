{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M&M model for fine-mapping\n",
    "\n",
    "This is the 5th version of M&M attempt. The formulation of this version was inspired by conditional regression for fine-mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bs}[1]{\\boldsymbol{#1}}$\n",
    "$\\DeclareMathOperator*{\\diag}{diag}$\n",
    "$\\DeclareMathOperator*{\\cov}{cov}$\n",
    "$\\DeclareMathOperator*{\\rank}{rank}$\n",
    "$\\DeclareMathOperator*{\\var}{var}$\n",
    "$\\DeclareMathOperator*{\\tr}{tr}$\n",
    "$\\DeclareMathOperator*{\\veco}{vec}$\n",
    "$\\DeclareMathOperator*{\\uniform}{\\mathcal{U}niform}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\ min}$\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\ max}$\n",
    "$\\DeclareMathOperator*{\\N}{N}$\n",
    "$\\DeclareMathOperator*{\\gm}{Gamma}$\n",
    "$\\DeclareMathOperator*{\\dif}{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M&M ASH model\n",
    "\n",
    "We assume the following multivariate, multiple regression model with $N$ samples, $J$ effects and $R$ conditions (and **without covariates, for the time being**)\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N\\times R} = \\bs{X}_{N \\times J}\\bs{B}_{J \\times R} + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Sigma}),\\\\\n",
    "\\bs{\\Sigma} &= \\diag(\\sigma_1,\\ldots,\\sigma_R).\n",
    "\\end{align}\n",
    "\n",
    "Let $\\bs{\\Lambda} = \\bs{\\Sigma}^{-1}$, we place Gamma prior on $\\bs{\\Lambda}$\n",
    "\n",
    "$$\\lambda_r \\overset{iid}{\\sim} \\gm(\\alpha, \\beta),$$\n",
    "\n",
    "and as a first path we set $\\alpha = \\beta = 0$ so that it is equivalent to estimating $\\bs{\\Sigma}$ via maximum likelihood.\n",
    "\n",
    "Let $\\omega_j := p(\\zeta_j)$ be the prior probability that effect $j$ is non-zero. We assume non-zero effects $\\bs{b}_j$ (rows of $\\bs{B}$) are iid with prior distribution of mixtures of multivariate normals\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_j|\\zeta_j = 1) = \\sum_{p = 0}^P\\pi_p\\N_R(\\bs{b}_j | \\bs{0}, \\bs{V}_p),\n",
    "\\end{align}\n",
    "\n",
    "where the $\\bs{V}_p$'s are $R \\times R$ positive semi-definite covariance matrices and the $\\pi_p$'s are their weights. We can augment the prior of $\\bs{b}_j$ by indicator vector $\\bs{\\gamma}_j \\in \\mathbb{R}^P$ denoting membership of $\\bs{b}_j$ into one of the $P$ mixture groups and write\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_j|\\zeta_j = 1, \\bs{\\gamma}_j) &= \\prod_{p = 0}^P\\left[\\N(\\bs{b}_j|\\bs{0},\\bs{V}_p)\\right]^{\\gamma_{jp}},\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{\\gamma}_j) &= \\prod_{p = 0}^{P} \\pi_p^{\\gamma_{jp}}\n",
    "\\end{align}\n",
    "\n",
    "Let $Z := (\\bs{B}, \\bs{\\zeta}, \\bs{\\Gamma})$, $\\Theta := (\\bs{\\omega}, \\bs{\\pi}, \\bs{V})$.\n",
    "The densities involved are\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{Y}, \\bs{\\Sigma}, \\bs{B},\\bs{\\zeta}, \\bs{\\Gamma}, \\bs{\\omega}, \\bs{\\pi}, \\bs{V}) &= \n",
    "p(\\bs{Y}, \\bs{\\Sigma}, Z, \\Theta) \\\\\n",
    "& = p(\\bs{Y}|\\bs{\\Sigma}, Z, \\Theta) p(Z|\\Theta) p(\\bs{\\Sigma})\n",
    "\\end{align}\n",
    "\n",
    "We are interested in computing the posterior $p(Z|\\Theta, \\bs{Y}, \\bs{\\Sigma})$. Note that in solving the M&M model below we assume $V_p$'s and their corresponding $\\pi_p$'s are known; in practice we use `mashr` to estimate these quantities and provide them to M&M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence lower bound (ELBO)\n",
    "Following from [the VEM framework](https://gaow.github.io/mvarbvs/writeup/20171203_VEM.html), \n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\bs{Y}|\\bs{\\Sigma}, \\Theta) & = E_q[\\log p(\\bs{Y} | Z, \\bs{\\Sigma}, \\Theta)] + \n",
    "E_q[\\log\\frac{p(Z|\\Theta)}{q(Z)}] - E_q[\\log \\frac{p(Z|\\bs{Y}, \\bs{\\Sigma}, \\Theta)}{q(Z)}] \\\\\n",
    "& = \\mathcal{L}(q, \\bs{\\Sigma}, \\Theta) + D_{kl}[q(Z) || p(Z|\\bs{Y}, \\bs{\\Sigma}, \\Theta)]\n",
    "\\end{align}\n",
    "\n",
    "where $D_{kl}(\\cdot)$ is the Kullbackâ€“Leibler (KL) divergence, and $\\mathcal{L}(q, \\bs{\\Sigma}, \\Theta) \\ge 0$ is Evidence lower bound (ELBO)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One effect model\n",
    "\n",
    "To derive the variational algorithm for fine-mapping with M&M we first discuss the case when there is only one non-zero effect, then show that the results can be generalized to the case with multiple non-zero effects to natually yield fine-mapping solutions. \n",
    "\n",
    "In the event where only one row $\\bs{B}_{1\\cdot}$ is non-zero, that is, $\\zeta_1 = 1$, $\\bs{\\zeta}_{-1} = 0$, M&M becomes\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N \\times R} = \\bs{x}\\bs{b}_1 + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "\n",
    "a multivariate, single regressor Bayesian regression with prior\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_1) = \\sum_{p = 0}^P\\pi_p\\N_R(\\bs{b}_1 | \\bs{0}, \\bs{V}_p).\n",
    "\\end{align}\n",
    "\n",
    "Let \"$\\propto$\" denote equality up to an additive constant independent of $q$, we write ELBO of this model\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_1(q, \\bs{\\Sigma}, \\Theta) &= \n",
    "E_q [\\log p(\\bs{Y} | \\bs{b}_1, \\bs{\\gamma}_1, \\bs{\\Sigma}, \\Theta)] + \n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1 | \\Theta)}{q(\\bs{b}_1, \\bs{\\gamma}_1)}] \\\\\n",
    "&= -\\frac{NR}{2}\\log(2\\pi) - \n",
    "\\frac{N}{2}E_q[\\log\\det (\\bs{\\Sigma})] - \n",
    "\\frac{1}{2}E_q \\{ \\tr[(\\bs{Y} - \\bs{xb_1}) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{xb_1})^\\intercal] \\} +\n",
    "E_q[\\log \\frac{p(\\bs{b}_1 | \\bs{\\gamma}_1, \\Theta)p(\\bs{\\gamma}_1)}{q(\\bs{b}_1 | \\bs{\\gamma}_1)q(\\bs{\\gamma_1})}]\n",
    "\\end{align}\n",
    "\n",
    "We focus on $E_q \\{ \\tr[(\\bs{Y} - \\bs{xb_1}) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{xb_1})^\\intercal] \\}$,\n",
    "\n",
    "\\begin{align}\n",
    "E_q \\{ \\tr[(\\bs{Y} - \\bs{xb_1}) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{xb_1})^\\intercal] \\} &= \n",
    "\\tr\\{\\bs{Y} E_q[\\bs{\\Sigma}^{-1}] \\bs{Y}^\\intercal\\} - \n",
    "2\\tr\\{\\bs{Y}E_q[\\bs{\\Sigma}^{-1}]E_q[\\bs{b}_1]^{\\intercal}\\bs{x}^\\intercal\\} +\n",
    "E_q[\\tr(\\bs{x}\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}^\\intercal)] \\\\\n",
    "& \\propto E_q[\\tr(\\bs{x}\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}^\\intercal)] - 2\\tr(\\bs{Y}\\bs{\\Sigma}^{-1}E_q[\\bs{b}_1]^\\intercal\\bs{x}^\\intercal)\n",
    "\\end{align}\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_1(q, \\bs{\\Sigma}, \\Theta) & \\propto\n",
    "E_q[\\tr(\\bs{x}\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}^\\intercal)] - 2\\tr(\\bs{Y}\\bs{\\Sigma}^{-1}E_q[\\bs{b}_1]^\\intercal\\bs{x}^\\intercal) +\n",
    "E_q[\\log \\frac{p(\\bs{b}_1 | \\bs{\\gamma}_1, \\Theta)p(\\bs{\\gamma}_1)}{q(\\bs{b}_1 | \\bs{\\gamma}_1)q(\\bs{\\gamma_1})}]\n",
    "\\end{align}\n",
    "\n",
    "In this case, we maximize $\\mathcal{L}_1(q, \\bs{\\Sigma}, \\Theta)$ by setting variational distribution to the posterior, since it can be calculated easily (FIXME: detail the computation here, along the lines of Wakefield 2009, also possibly as implemented in `mashr` package). That is, $p(Z|\\Theta, \\bs{Y}, \\bs{\\Sigma}) = \\argmax \\mathcal{L}_1(q, \\bs{\\Sigma}, \\Theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two effects model\n",
    "\n",
    "In the event where two rows of $\\bs{B}$ is non-zero, that is, $\\zeta_1 = \\zeta_2 = 1$, $\\bs{\\zeta}_{-1, -2} = 0$, M&M becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N \\times R} = \\bs{x}\\bs{b}_1 + \\bs{x}\\bs{b}_2 + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "\n",
    "a multivariate, two regressor Bayesian regression with independent priors $p(\\bs{b}_1, \\bs{b}_2) = p(\\bs{b}_1)p(\\bs{b}_2)$ where \n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_\\cdot) = \\sum_{p = 0}^P\\pi_p\\N_R(\\bs{b}_\\cdot | \\bs{0}, \\bs{V}_p).\n",
    "\\end{align}\n",
    "\n",
    "we write ELBO of this model\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_2(q, \\bs{\\Sigma}, \\Theta) &= \n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
