{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "source": [
    "# M&M model for fine-mapping\n",
    "\n",
    "This is the 5th version of M&M. The formulation of this version was inspired by conditional regression commonly used in fine-mapping, as discussed in [T Flutre et al 2013](https://doi.org/10.1371/journal.pgen.1003486)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\bs}[1]{\\boldsymbol{#1}}$\n",
    "$\\DeclareMathOperator*{\\diag}{diag}$\n",
    "$\\DeclareMathOperator*{\\cov}{cov}$\n",
    "$\\DeclareMathOperator*{\\rank}{rank}$\n",
    "$\\DeclareMathOperator*{\\var}{var}$\n",
    "$\\DeclareMathOperator*{\\tr}{tr}$\n",
    "$\\DeclareMathOperator*{\\veco}{vec}$\n",
    "$\\DeclareMathOperator*{\\uniform}{\\mathcal{U}niform}$\n",
    "$\\DeclareMathOperator*{\\argmin}{arg\\ min}$\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\ max}$\n",
    "$\\DeclareMathOperator*{\\N}{N}$\n",
    "$\\DeclareMathOperator*{\\gm}{Gamma}$\n",
    "$\\DeclareMathOperator*{\\dif}{d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M&M ASH model\n",
    "\n",
    "We assume the following multivariate, multiple regression model with $N$ samples, $J$ effects and $R$ conditions (and *without covariates, which will be discussed separately*)\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N\\times R} = \\bs{X}_{N \\times J}\\bs{B}_{J \\times R} + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Sigma}).\n",
    "\\end{align}\n",
    "\n",
    "As a first path we assume $\\bs{\\Sigma}$ is known (or use estimates from elsewhere).\n",
    "\n",
    "Let $\\omega_j := p(\\alpha_j)$ be the prior probability that effect $j$ is non-zero,\n",
    "\n",
    "\\begin{align}\n",
    "\\alpha_j \\sim \\text{Bernoulli}(\\omega_j)\n",
    "\\end{align}\n",
    "\n",
    "We assume non-zero effects $\\bs{b}_j$ (rows of $\\bs{B}$) are iid with prior distribution of mixtures of multivariate normals\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_j|\\alpha_j = 1) = \\sum_{p = 0}^P\\pi_{jp}\\N_R(\\bs{b}_j | \\bs{0}, \\bs{V}_p),\n",
    "\\end{align}\n",
    "\n",
    "where the $\\bs{V}_p$'s are $R \\times R$ positive semi-definite covariance matrices and are known, with corresponding weights $\\pi_{\\cdot,p}$'s to be estimated ([Urbut et al 2017](https://www.biorxiv.org/content/early/2017/05/09/096552)). We can augment the prior of $\\bs{b}_j$ by indicator vector $\\bs{\\gamma}_j \\in \\mathbb{R}^P$ denoting membership of $\\bs{b}_j$ into one of the $P$ mixture groups and write\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_j|\\alpha_j = 1, \\bs{\\gamma}_j) &= \\prod_{p = 0}^P\\left[\\N(\\bs{b}_j|\\bs{0},\\bs{V}_p)\\right]^{\\gamma_{jp}},\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{\\gamma}_j) &= \\prod_{p = 0}^{P} \\pi_{jp}^{\\gamma_{jp}}\n",
    "\\end{align}\n",
    "\n",
    "The densities involved are\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{Y}, \\bs{B},\\bs{\\alpha}, \\bs{\\Gamma} | \\bs{\\Sigma}, \\bs{\\omega}, \\bs{\\Pi}) & = \n",
    "p(\\bs{Y}|\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}, \\bs{\\omega}, \\bs{\\Pi}, \\bs{\\Sigma}) p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma} |\\bs{\\omega}, \\bs{\\Pi}) \\\\\n",
    "&= p(\\bs{Y}|\\bs{B}, \\bs{\\Sigma}) p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma} |\\bs{\\omega}, \\bs{\\Pi}) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational EM and evidence lower bound (ELBO)\n",
    "Following from [the VEM framework](https://gaow.github.io/mvarbvs/writeup/20171203_VEM.html) where $Z:=(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma})$ and $\\theta:= (\\bs{\\Sigma}, \\bs{\\Pi}, \\bs{\\omega})$, \n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\bs{Y}|\\bs{\\Sigma}, \\bs{\\pi}, \\bs{\\omega}) & \\ge  \\mathcal{L}(q, \\bs{\\Sigma}, \\bs{\\Pi}, \\bs{\\omega}) \\\\\n",
    "&= E_q[\\log p(\\bs{Y}|\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}, \\bs{\\omega}, \\bs{\\Pi}, \\bs{\\Sigma})] + \n",
    "E_q[\\log\\frac{p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}| \\bs{\\Sigma}, \\bs{\\Pi}, \\bs{\\omega})}{q(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma})}] \\\\\n",
    "&= E_q[\\log p(\\bs{Y}|\\bs{B}, \\bs{\\Sigma})] + \n",
    "E_q[\\log\\frac{p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}|\\bs{\\Sigma}, \\bs{\\Pi}, \\bs{\\omega})}{\\prod_j q(\\bs{b}_j, \\alpha_j, \\bs{\\gamma}_j)}],\n",
    "\\end{align}\n",
    "\n",
    "that is, we use a fully-fectorized variational approximation of posterior $q(\\cdot)$, thus performing mean-field variational inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative variational algorithm\n",
    "\n",
    "Inspired by the conditional regression approach for fine mapping, we present a new model,\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N\\times R} = \\bs{X}_{N \\times J}\\sum_l^L \\diag(\\bs{\\alpha}_l) {\\bs{B}_l}_{J \\times R} + \\bs{E}_{N \\times R},\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "where $L$ is the number of non-zero effects, or the upper bound on the number of non-zero effects if prior distribution of ${\\bs{b}_l}_j$ includes a point mass at zero. For each $l = 1,\\ldots, L$ we assume that *exactly 1 of the $J$ effects is non-zero*, as indicated by $\\alpha_{lj}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{\\alpha}_l \\sim \\text{Multinomial}(1, \\bs{\\omega}_l)\n",
    "\\end{align}\n",
    "\n",
    "The key idea behind this model is to use the following variational approximation,\n",
    "\n",
    "\\begin{align}\n",
    "q(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}) &= \\prod_l q(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l) \\\\\n",
    "&=  \\prod_l \\prod_j q({\\bs{b}_l}_{j}, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)\n",
    "\\end{align}\n",
    "\n",
    "Crucially, we do not factorize $q(\\bs{\\alpha}_l)$ across its $J$ elements, that is, $\\bs{\\alpha}_l$ is a binary vector with exactly one non-zero element. We will reveal connection with conditional regression later as we develop the variational algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of variational M&M\n",
    "\n",
    "### One effect model\n",
    "\n",
    "To develop the variational algorithm for fine-mapping with M&M we first discuss the case when there is only one non-zero effect, then show that the results can be generalized to the case with multiple non-zero effects to natually yield fine-mapping solutions. \n",
    "\n",
    "In the event where only one row $\\bs{B}_{1\\cdot}$ is non-zero, that is, $\\omega_1 = 1$, $\\bs{\\omega}_{-1} = \\bs{0}$, M&M becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N \\times R} = \\bs{x}_1\\bs{b}_1 + \\bs{E}_{N \\times R}, \\quad \\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Sigma}),\n",
    "\\end{align}\n",
    "\n",
    "a multivariate, single regressor Bayesian regression with prior\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_1, \\bs{\\gamma}_1) = \\sum_{p = 0}^P\\pi_{1p}\\N_R(\\bs{b}_1 | \\bs{0}, \\bs{V}_p).\n",
    "\\end{align}\n",
    "\n",
    "Let \"$\\propto$\" denote equality up to an additive constant independent of $q$, we write ELBO of this model\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_1(q, \\bs{\\Sigma}, \\bs{\\pi}_1; \\bs{Y}) &= \n",
    "E_q [\\log p(\\bs{Y} | \\bs{b}_1, \\bs{\\Sigma})] + \n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1 | \\bs{\\pi}_1)}{q(\\bs{b}_1, \\bs{\\gamma}_1)}] \\\\\n",
    "&= -\\frac{NR}{2}\\log(2\\pi) - \n",
    "\\frac{N}{2}E_q[\\log\\det (\\bs{\\Sigma})] - \n",
    "\\frac{1}{2}E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1 \\bs{b}_1) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1 \\bs{b}_1)^\\intercal] \\} +\n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1 | \\bs{\\pi}_1)}{q(\\bs{b}_1, \\bs{\\gamma}_1)}]\n",
    "\\end{align}\n",
    "\n",
    "We focus on $E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1\\bs{b}_1) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1\\bs{b}_1)^\\intercal] \\}$,\n",
    "\n",
    "\\begin{align}\n",
    "E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1\\bs{b}_1) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1\\bs{b}_1)^\\intercal] \\} &= \n",
    "\\tr\\{\\bs{Y} E_q[\\bs{\\Sigma}^{-1}] \\bs{Y}^\\intercal\\} - \n",
    "2\\tr\\{\\bs{Y}E_q[\\bs{\\Sigma}^{-1}]E_q[\\bs{b}_1]^{\\intercal}\\bs{x}_1^\\intercal\\} +\n",
    "E_q[\\tr(\\bs{x}_1\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}_1^\\intercal)] \\\\\n",
    "& \\propto E_q[\\tr(\\bs{x}_1\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}_1^\\intercal)] - 2\\tr(\\bs{Y}\\bs{\\Sigma}^{-1}E_q[\\bs{b}_1]^\\intercal\\bs{x}_1^\\intercal)\n",
    "\\end{align}\n",
    "\n",
    "Therefore, \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_1(q, \\bs{\\Sigma}, \\bs{\\pi}_1; \\bs{Y}) & \\propto\n",
    "E_q[\\tr(\\bs{x}_1\\bs{b}_1\\bs{\\Sigma}^{-1}\\bs{b}^\\intercal \\bs{x}_1^\\intercal)] - 2\\tr(\\bs{Y}\\bs{\\Sigma}^{-1}E_q[\\bs{b}_1]^\\intercal\\bs{x}_1^\\intercal) +\n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1 | \\bs{\\pi}_1)}{q(\\bs{b}_1, \\bs{\\gamma}_1)}]\n",
    "\\end{align}\n",
    "\n",
    "In this case, we maximize $\\mathcal{L}_1(q, \\bs{\\Sigma}, \\bs{\\pi}_1; \\bs{Y})$ by simply setting variational distribution $q(\\cdot)$ to the posterior, \n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_1, \\bs{\\gamma}_1 | \\bs{Y}, \\bs{\\Sigma}, \\bs{\\pi}_1) = \\argmax \\mathcal{L}_1(q, \\bs{\\Sigma}, \\bs{\\pi}_1; \\bs{Y})\n",
    "\\end{align}\n",
    "\n",
    "since it can be calculated relatively easily due to established work, as will be revealed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two effects model\n",
    "\n",
    "In the event where two rows of $\\bs{B}$ is non-zero, that is, $\\omega_1 = \\omega_2 = 1$, $\\bs{\\omega}_{-1, -2} = \\bs{0}$, M&M becomes\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N \\times R} = \\bs{x}_1\\bs{b}_1 + \\bs{x}_2\\bs{b}_2 + \\bs{E}_{N \\times R}, \\quad \\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Sigma}),\n",
    "\\end{align}\n",
    "\n",
    "a multivariate, two regressor Bayesian regression with independent priors $p(\\bs{b}_1, \\gamma_1, \\bs{b}_2, \\gamma_2) = p(\\bs{b}_1,\\gamma_1)p(\\bs{b}_2,\\gamma_2)$ where \n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}_\\cdot, \\bs{\\gamma}_\\cdot) = \\sum_{p = 0}^P\\pi_p\\N_R(\\bs{b}_\\cdot | \\bs{0}, \\bs{V}_p).\n",
    "\\end{align}\n",
    "\n",
    "we write ELBO of this model\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_2(q, \\bs{\\Sigma}, \\bs{\\pi}_1, \\bs{\\pi}_2; \\bs{Y}) &= \n",
    "E_q [\\log p(\\bs{Y} | \\bs{b}_1, \\bs{b}_2, \\bs{\\Sigma})] + \n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2)}{q(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2)}] \\\\\n",
    "&= -\\frac{NR}{2}\\log(2\\pi) - \n",
    "\\frac{N}{2}E_q[\\log\\det (\\bs{\\Sigma})] - \n",
    "\\frac{1}{2}E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2)^\\intercal] \\} +\n",
    "E_q[\\log \\frac{p(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2 | \\bs{\\pi_1}, \\bs{\\pi_2})}{q(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2)}]\n",
    "\\end{align}\n",
    "\n",
    "where we choose $q(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2) = q_1(\\bs{b}_1, \\bs{\\gamma}_1)q_2(\\bs{b}_2, \\bs{\\gamma}_2)$ be the variational approximation to the posterior $p(\\bs{b}_1, \\bs{\\gamma}_1, \\bs{b}_2, \\bs{\\gamma}_2|\\bs{Y}, \\bs{\\pi_1}, \\bs{\\pi_2}, \\bs{\\Sigma})$, ie, a \"fully factorized\" variational approximation. This allows us to use an iterative approach to maximize the ELBO.\n",
    "\n",
    "#### Maximize over $q_2$ with $q_1$ fixed\n",
    "\n",
    "Similar to the \"one effect model\" we focus on\n",
    "\n",
    "\\begin{align}\n",
    "E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2)^\\intercal] \\},\n",
    "\\end{align}\n",
    "\n",
    "and analogous to the setup of \"conditional regression\", we treat $q_1$ fixed and maximize over $q_2$ only,\n",
    "\n",
    "\\begin{align}\n",
    "E_q \\{ \\tr[(\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2) \\bs{\\Sigma}^{-1} (\\bs{Y} - \\bs{x}_1\\bs{b}_1 - \\bs{x}_2\\bs{b}_2)^\\intercal] \\} & \\propto\n",
    "E_{q_2}\\{\\tr(\\bs{x}_2\\bs{b}_2\\bs{\\Sigma}^{-1}\\bs{b}_2^\\intercal\\bs{x}_2^\\intercal)\\} -\n",
    "2\\tr\\{(\\bs{Y} - \\bs{x}_1E_{q_1}[\\bs{b}_1])\\bs{\\Sigma}^{-1}E_{q_2}[\\bs{b}_2]^\\intercal \\bs{x}_2^\\intercal\\}\n",
    "\\end{align}\n",
    "\n",
    "Let $\\bs{\\xi}_1 = \\bs{Y} - \\bs{x}_1E_{q_1}[\\bs{b}_1]$, we have\n",
    "\\begin{align}\n",
    "\\mathcal{L}_2(q_2, \\bs{\\Sigma}, \\bs{\\pi}_2; \\bs{Y}) & \\propto E_{q_2}\\{\\tr(\\bs{x}_2\\bs{b}_2\\bs{\\Sigma}^{-1}\\bs{b}_2^\\intercal\\bs{x}_2^\\intercal)\\} -\n",
    "2\\tr\\{\\bs{\\xi}_1\\bs{\\Sigma}^{-1}E_{q_2}[\\bs{b}_2]^\\intercal \\bs{x}_2^\\intercal\\} +\n",
    "E_{q_2}[\\log \\frac{p(\\bs{b}_2, \\bs{\\gamma}_2 |\\bs{\\pi}_2)}{q_2(\\bs{b}_2, \\bs{\\gamma}_2)}],\n",
    "\\end{align}\n",
    "\n",
    "and similar to the case with \"one effect model\", $p(\\bs{b}_2, \\bs{\\gamma}_2|\\bs{\\xi}_1, \\bs{\\pi}_2, \\bs{\\Sigma}) = \\argmax \\mathcal{L}_2(q_2, \\bs{\\Sigma}, \\bs{\\pi}_2; \\bs{Y})$. In other words we maximize $\\mathcal{L}_2$ over $q_2$ with $q_1$ fixed, by applying the same posterior computation for maximizing $\\mathcal{L}_1$ over $q_1$ but using residualized response $\\bs{\\xi}_1$ rather than the original response $\\bs{Y}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_2(q_2, \\bs{\\Sigma}, \\bs{\\pi}_2; \\bs{Y}) \\propto \\mathcal{L}_1(q_2, \\bs{\\Sigma}, \\bs{\\pi}_2; \\bs{\\xi}_1)\n",
    "\\end{align}\n",
    "\n",
    "#### Maximize over $q_1$ with $q_2$ fixed\n",
    "\n",
    "Similarly we can maximize $\\mathcal{L}_1$ over $q_1$ with $q_2$ fixed. The algorithm iterates until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An iterative algorithm: generalization to arbitary number of effects\n",
    "\n",
    "The arguments above can be generalized to having $L$ non-zero effects. For the $l$-th effect we optimize $\\mathcal{L}_l$ over $q_l$ with all other $q_{-l}$ fixed, by applying Bayesian multivariate regression to $\\bs{\\xi}_{-l} = \\bs{x}_l \\bs{b}_l + E $. The algorithm iterates until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core computation of the iterative algorithm\n",
    "\n",
    "#### Posterior of multivariate analysis\n",
    "\n",
    "As previously discussed, the core computation is making inference on Bayesian multivariate regression \n",
    "\n",
    "\\begin{align}\n",
    "\\bs{Y}_{N \\times R} = \\bs{x}_{N \\times 1}\\bs{b}_{1 \\times R} + \\bs{E}_{N \\times R}, \\quad \\bs{E} &\\sim \\N_{N \\times R}(\\bs{0}, \\bs{I}_N, \\bs{\\Sigma}),\n",
    "\\end{align}\n",
    "\n",
    "with prior\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}, \\bs{\\gamma}) = \\sum_{p = 0}^P\\pi_{p}\\N_R(\\bs{b} | \\bs{0}, \\bs{V}_p).\n",
    "\\end{align}\n",
    "\n",
    "We compute OLS estimates for $\\bs{b}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\bs{B}}_{jr} & = (\\bs{x}_j^\\intercal\\bs{x}_j)^{-1}(\\bs{X}^\\intercal\\bs{Y})_{jr} \\\\\n",
    "& = (\\bs{x}_j^\\intercal\\bs{x}_j)^{-1}\\bs{x}_j\\bs{y}_{r}\n",
    "\\end{align}\n",
    "\n",
    "We assume \n",
    "\\begin{align}\n",
    "\\bs{\\Sigma} = \\bs{D}\\bs{C}\\bs{D}\n",
    "\\end{align}\n",
    "\n",
    "where $\\bs{D} = \\diag(\\sigma_1, \\ldots, \\sigma_R)$ and $\\bs{C}$ is a given constant matrix. Choices of $\\bs{C}$ can be some estimate of correlations between conditions eg, $\\bs{C} = \\text{cor}(\\bs{Y})$, or simply $\\bs{C} = \\bs{I}_R$. We plug-in $\\hat{\\sigma}^2_r = \\text{Var}(\\bs{y}_r)$, and estimate standard error for $\\hat{\\bs{b}}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\bs{S}} = (\\bs{x}_j^\\intercal\\bs{x}_j)^{-1} \\hat{\\bs{\\Sigma}}\n",
    "\\end{align}\n",
    "\n",
    "Alternatively we can also possibly estimate $\\bs{\\Sigma}$ by computing $E[\\frac{1}{N}(\\bs{Y}-\\bs{Xb})^\\intercal (\\bs{Y}-\\bs{Xb})]$, which involves dealing with 2nd posterior moment (see formula ?? below in ELBO calculation section). It is doable but we argue that using plug-in estimate is potentially more beneficial as one can provide better pre-computed $\\bs{C}$.\n",
    "\n",
    "The likelihood can be formulated using summary statistics,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\bs{b}} | \\bs{b} \\sim N_R(\\bs{b}, \\hat{\\bs{S}})\n",
    "\\end{align}\n",
    "\n",
    "The conjugate prior leads us to the posterior distributed also as a mixture of multivariate normal,\n",
    "\n",
    "\\begin{align}\n",
    "p(\\bs{b}|\\hat{\\bs{b}}, \\hat{\\bs{S}}, \\hat{\\bs{\\pi}}) & = \\sum_p p(\\bs{b}|\\hat{\\bs{b}}, \\hat{\\bs{S}}, \\gamma_p)p(\\gamma_p |\\hat{\\bs{b}}, \\hat{\\bs{S}}, \\hat{\\pi}_p) \\\\\n",
    "&= \\sum_p p(\\bs{b}|\\hat{\\bs{b}}, \\hat{\\bs{S}}, \\gamma_p) \\tilde{\\pi}_p,\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{\\bs{\\pi}}$ for the Gaussian mixture can be estimated using established procedures ([Urbut et al 2017](https://www.biorxiv.org/content/early/2017/05/09/096552)). For a multivariate normal distrubtion, [we know that](https://learnbayes.org/index.php?option=com_content&view=article&id=77:completesquare&catid=83&Itemid=479&showall=1&limitstart=) the posterior on $\\bs{b}|\\hat{\\bs{b}}, \\hat{\\bs{S}}, \\gamma_p$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\bs{b} | \\hat{\\bs{b}}, \\hat{\\bs{S}}, \\gamma_p \\sim N_R(\\bar{\\bs{b}}_p, \\bs{U}_p)\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "\\hat{\\bar{\\bs{b}}}_p &= \\bs{U}_p(\\hat{\\bs{S}}^{-1}\\hat{\\bs{b}})\\\\\n",
    "\\hat{\\bs{U}}_p &= (\\bs{V}_p^{-1} + \\hat{\\bs{S}}^{-1})^{-1} \\\\\n",
    "&= ((\\bs{V}_p^{-1} + \\hat{\\bs{S}}^{-1})\\bs{V}_p\\bs{V}_p^{-1})^{-1} \\\\\n",
    "&= ((\\bs{I} + \\hat{\\bs{S}}^{-1}\\bs{V}_p)\\bs{V}_p^{-1})^{-1} \\\\\n",
    "&= \\bs{V}_p(\\bs{I} + \\hat{\\bs{S}}^{-1}\\bs{V}_p)^{-1}\n",
    "\\end{align}\n",
    "\n",
    "We also know that marginal likelihood of $\\hat{\\bs{b}}$ in this setting is also normal (cite *Statistical decision theory and Bayesian analysis 2nd edition by James Berger, Section 4.2 Example 1*), with\n",
    "\n",
    "\\begin{align}\n",
    "E[\\hat{\\bs{b}} | \\hat{\\bs{S}}, \\gamma_p] &= E_{\\bs{b}}\\big [E[\\hat{\\bs{b}} | \\hat{\\bs{S}}, \\gamma_p, \\bs{b}]\\big ] \\\\\n",
    "&= E_{\\bs{b}}[\\bs{b}|\\gamma_p] \\\\\n",
    "&= \\bs{0}, \\\\\n",
    "Var(\\hat{\\bs{b}} | \\hat{\\bs{S}}, \\gamma_p) &= E_{\\bs{b}}\\big [Var[\\hat{\\bs{b}} | \\hat{\\bs{S}}, \\gamma_p, \\bs{b}]\\big ] + Var_{\\bs{b}}\\big [E[\\hat{\\bs{b}} | \\hat{\\bs{S}}, \\gamma_p, \\bs{b}]\\big ] \\\\\n",
    "&= E_{\\bs{b}}[\\hat{\\bs{S}}] + Var_{\\bs{b}}[\\bs{b}|\\gamma_p] \\\\\n",
    "&= \\hat{\\bs{S}} + \\bs{V}_{p}\n",
    "\\end{align}\n",
    "\n",
    "The posterior weights $\\tilde{\\pi}_p$ can the be estimated by \n",
    "\\begin{align}\n",
    "\\hat{\\tilde{\\pi}}_{p} &= \\frac{\\hat{\\pi}_p N(\\hat{\\bs{b}}; \\bs{0}, \\hat{\\bs{S}} + \\bs{V}_{p})}{\\sum_p \\hat{\\pi}_p N(\\hat{\\bs{b}}; \\bs{0}, \\hat{\\bs{S}} + \\bs{V}_{p})}\n",
    "\\end{align}\n",
    "\n",
    "$\\tilde{\\bs{b}}$, posterior mean of $\\bs{b}$ is then estimated by $\\hat{\\tilde{\\bs{b}}} = \\sum_p \\hat{\\tilde{\\pi}}_p \\bar{\\bs{b}}_p$.\n",
    "\n",
    "To compute $\\tilde{\\bs{S}}$, posterior standard deviation of $\\bs{b}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\bs{S}} = \\sqrt{Var(\\bs{b})} &= \\sqrt{E[\\bs{b} \\bs{b}^\\intercal] - E[\\bs{b}]E[\\bs{b}]^\\intercal} \\\\\n",
    "&= \\sqrt{\\tilde{\\bs{b}^2} - \\tilde{\\bs{b}}\\tilde{\\bs{b}}^\\intercal}\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\bs{b}^2} &:= \\sum_p \\tilde{\\pi}_p \\bar{\\bs{b}^2}_p \\\\\n",
    "\\bar{\\bs{b}^2}_p &:= \\bar{\\bs{b}}_p\\bar{\\bs{b}}_p^\\intercal + \\bs{U}_p\n",
    "\\end{align}\n",
    "\n",
    "#### Bayes factor\n",
    "\n",
    "Bayes factor is the ratio of the likelihood of alternative vs null,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{BF} = \\frac{\\sum_p \\hat{\\pi}_p N(\\hat{\\bs{b}}; \\bs{0}, \\hat{\\bs{S}} + \\bs{V}_{p})}{N(\\hat{\\bs{b}}; \\bs{0}, \\hat{\\bs{S}})}\n",
    "\\end{align}\n",
    "\n",
    "#### Local false sign rate (lfsr)\n",
    "\n",
    "To measure statistical significance of estimated effect we use local false sign rate, defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{lfsr}_{jr} := \\text{min}[p(\\tilde{\\beta}_{jr} \\ge 0), p(\\tilde{\\beta}_{jr} \\le 0)]\n",
    "\\end{align}\n",
    "\n",
    "Similar to (but more conservative than) local false discovery rate (lfdr), a small lfsr indicates high confidence in the sign of an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational updates for quantities of interest\n",
    "\n",
    "As indicated by our model, define $\\bs{B}:=\\sum_l \\diag(\\bs{\\alpha}_l)\\bs{B}_l$, we want to compute $\\tilde{\\bs{B}}$, the variational mean for $\\bs{B}$ and $\\bs{\\rho}: = \\{\\bs{\\rho}_j\\}$, a $J$ vector of $R \\times R$ matrix for variational standard deviation of each row of $\\bs{B}$.\n",
    "\n",
    "#### Posterior inference\n",
    "\n",
    "Applying the core computation for multivariate regression in the iterative algorithm to each $l$ and $j$ as derived above, $\\tilde{\\bs{B}}_l$ and $\\bs{\\rho}_l$ can be estimated. \n",
    "Posterior mean of the $J$ vector $\\bs{\\alpha}_l$ can be estimated by Bayes Factor averaging:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\tilde{{\\omega_l}}}_j = \\frac{{\\omega_l}_j{\\text{BF}_l}_j}{\\sum_j {\\omega_l}_j{\\text{BF}_l}_j}\n",
    "\\end{align}\n",
    "\n",
    "where, as a first pass, we can let $\\bs{\\omega}_l$ be uniform. $\\tilde{\\bs{B}}$ is therefore estimated by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\tilde{\\bs{B}}} = \\sum_l \\diag(\\hat{\\tilde{\\bs{\\omega}_l}}) \\hat{\\tilde{\\bs{B}_l}}\n",
    "\\end{align}\n",
    "\n",
    "$\\bs{\\rho}_j$, variational standard deviation of $\\bs{b}_j$, is computed by\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\bs{\\rho}}_j = \\sqrt{\\tilde{\\bs{b}_j^2} - \\tilde{\\bs{b}_j} \\tilde{\\bs{b}_j}^\\intercal}\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\bs{b}_j^2} &:= \\sum_l \\tilde{\\omega_j}_l \\tilde{\\bs{b}_j^2}_l \\\\\n",
    "\\tilde{\\bs{b}_j^2}_l &:= \\tilde{\\bs{b}_j}_l\\tilde{\\bs{b}_j}_l^\\intercal  + {\\tilde{\\bs{S}}^2_j}_l\n",
    "\\end{align}\n",
    "\n",
    "#### Local false sign rate (lfsr)\n",
    "\n",
    "For each of the $l$-th fit, we have previously obtained lfsr for effect $j$ under condition $r$. To collectively measure statistical significance of the set of effects discovered in the $l$-th fit, we define lfsr as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "{\\text{lfsr}_l}_r & := 1 - \\sum_j \\tilde{{\\omega_l}}_j (1-\\text{lfsr}_{jr}) \\\\\n",
    "& = \\sum_j \\tilde{{\\omega_l}}_j \\text{lfsr}_{jr}\n",
    "\\end{align}\n",
    "\n",
    "A smaller lfsr indicates high confidence in the $l$-th fit, that the sets of effects discovered in $l$-th fit are \"mappable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of ELBO\n",
    "\n",
    "ELBO for the new variational M&M model is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(q, \\bs{\\omega}, \\bs{\\Pi}, \\bs{\\Sigma}; \\bs{Y}) & = E_q[\\log p(\\bs{Y} | \\bs{B}, \\bs{\\alpha}, \\bs{\\Lambda}, \\bs{\\omega}, \\bs{\\Pi}, \\bs{\\Sigma})] + \n",
    "E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}] \\\\\n",
    "& = -\\frac{NR}{2}\\log(2\\pi) - \\frac{N}{2}\\log(\\det{\\bs{\\Sigma}}) - \\frac{1}{2} \\big\\{E_q\\{\\tr(\\bs{X}\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal)\\} - 2\\tr\\{\\bs{Y}\\bs{\\Sigma}^{-1}\\bar{\\bs{B}}^\\intercal\\bs{X}^\\intercal\\} + \\tr\\{\\bs{Y}\\bs{\\Sigma}^{-1}\\bs{Y}^\\intercal\\} \\big\\} +\n",
    "E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}] \n",
    "\\end{align}\n",
    "\n",
    "where $\\bs{B}:=\\sum_l \\diag(\\bs{\\alpha}_l)\\bs{B}_l$, $\\bar{\\bs{B}}:= E_q[\\bs{B}]$. \n",
    "\n",
    "We estimate $\\bar{\\bs{B}}$ using $\\hat{\\tilde{\\bs{B}}}$ as shown above. As a first pass we estimate $\\bs{\\Sigma}$ using $\\hat{\\bs{\\Sigma}} = \\frac{1}{N}(\\bs{Y} - \\bs{X}\\bar{\\bs{B}})^\\intercal (\\bs{Y} - \\bs{X}\\bar{\\bs{B}})$. We work out $E_q\\{\\tr(\\bs{X}\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal)\\}$ and $E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}]$ next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $E_q\\{\\tr(\\bs{X}\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal)\\}$\n",
    "\n",
    "\\begin{align}\n",
    "E_q\\{\\tr(\\bs{X}\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal)\\} &= \n",
    "E_q\\{\\tr(\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal\\bs{X})\\} \\quad \\text{Cyclic permutation of trace} \\\\\n",
    "&= E_q\\{\\tr(\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal \\bs{S} \\bs{B})\\} \\\\\n",
    "&= \\tr\\{\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\},\n",
    "\\end{align}\n",
    "\n",
    "where $\\bs{S}:=\\bs{X}^\\intercal\\bs{X}$. Now we focus on element-wise computations for $\\big(\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\big)$. Recall that $\\bs{B} \\in \\mathbb{R}^{J\\times R}$, $\\bs{S} \\in \\mathbb{R}^{J\\times J}$, $\\bs{\\Sigma}^{-1} \\in \\mathbb{R}^{R\\times R}$,\n",
    "\n",
    "\\begin{align}\n",
    "[\\bs{B}^\\intercal\\bs{S}]_{rj'} &= \\sum_j^J B_{jr} S_{jj'}, \\\\\n",
    "[\\bs{B}^\\intercal\\bs{S}\\bs{B}]_{rr'} &= \\sum_j \\sum_{j'} B_{jr} S_{jj'} B_{j'r'}, \\\\\n",
    "E_q[\\bs{B}^\\intercal\\bs{S}\\bs{B}]_{rr'} &= \\sum_j \\sum_{j'} S_{jj'} E_q[B_{jr} B_{j'r'}] \\\\\n",
    "&= \\sum_j \\sum_{j'} S_{jj'} E_q[B_{jr}] E_q[B_{j'r'}] + \\sum_j \\sum_{j'} S_{jj'} \\cov(B_{jr}, B_{j'r'}) \\\\\n",
    "&= \\sum_j \\sum_{j'} S_{jj'} \\bar{B}_{jr} \\bar{B}_{j'r'} + \\sum_j \\sum_{j'} S_{jj'} \\rho^2_{jrr'}\\mathbb{1}(j=j'),\n",
    "\\end{align}\n",
    "\n",
    "where $\\rho^2_{jrr'}:= \\cov(b_{jr}, b_{jr'})$ is non-zero for the $j$-th effect at conditions $r$ and $r'$, and can be computed by posterior standard deviation for $\\bs{b}_j$. For $j \\ne j'$, due to the model assumption of independent effects, the correlations are zero.\n",
    "\n",
    "The $rr'$ element of $\\big(\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\big)$ is thus\n",
    "\n",
    "\\begin{align}\n",
    "\\big(\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\big)_{rr'} &= \n",
    "\\sum_l^R \\lambda_{rl}\\big(\\sum_j \\sum_{j'}S_{jj'}\\bar{B}_{jl}\\bar{B}_{j'r'} + \\sum_j S_{jj}\\rho^2_{jlr'}\\big) \\\\\n",
    "& = \\sum_l^R\\sum_j\\sum_{j'}\\lambda_{rl}S_{jj'}\\bar{B}_{jl}\\bar{B}_{j'r'} + \n",
    "\\sum_l^R\\sum_j \\lambda_{rl} S_{jj} \\rho^2_{jlr'},\n",
    "\\end{align}\n",
    "\n",
    "where $\\lambda_{rl}:=[\\bs{\\Sigma}^{-1}]_{rl}$. Finally,\n",
    "\n",
    "\\begin{align}\n",
    "E_q\\{\\tr(\\bs{X}\\bs{B}\\bs{\\Sigma}^{-1}\\bs{B}^\\intercal\\bs{X}^\\intercal)\\} &= \n",
    "\\tr\\{\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\} \\\\\n",
    "&= \\sum_r^R\\{\\bs{\\Sigma}^{-1}E_q[\\bs{B}^\\intercal \\bs{S} \\bs{B}]\\} \\\\\n",
    "&= \\sum_r \\sum_l^R \\sum_j \\sum_{j'} \\lambda_{rl} S_{jj'} \\bar{B}_{jl} \\bar{B}_{j'r} + \n",
    "\\sum_r \\sum_l^R \\sum_j \\lambda_{rl} S_{jj} \\rho^2_{jlr} \\\\\n",
    "&= \\sum_r \\sum_{r'} \\sum_j \\sum_{j'}\\lambda_{rr'} S_{jj'} \\bar{B}_{jr'} \\bar{B}_{j'r} + \n",
    "\\sum_r \\sum_{r'} \\sum_j \\lambda_{rr'} S_{jj} \\rho^2_{jrr'} \\\\\n",
    "&= \\tr\\{\\bs{\\Sigma}^{-1}\\bar{\\bs{B}}^\\intercal \\bs{S} \\bar{\\bs{B}}\\} + \n",
    "\\sum_r \\sum_{r'} \\sum_j \\lambda_{rr'} S_{jj} \\rho^2_{jrr'}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}]$, method 1\n",
    "\n",
    "**Note: I did not realize until written in code that this approach does not work because $\\bs{V}_p$ can be low-rank thus the prior does not have a density. However in FLASH paper there is an alternative to computer this KL divergence using marginal log-likelihood and posterior expected log-likelihood from normal-means problem such as this (distribution of prior no longer matters). See method 2 for details**.\n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}] &= \n",
    "\\sum_l E_q[\\log p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)] - \\sum_l \\sum_j E_q[\\log q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)]\n",
    "\\end{align}\n",
    "\n",
    "Without loss of generality in the calculations below we drop the subscript $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $E_q[\\log p(\\bs{B},\\bs{\\alpha},\\bs{\\Gamma}|\\bs{\\Pi}, \\bs{\\omega})]$\n",
    "\n",
    "By Bishop 2006 Equation 9.36, \n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\log p(\\bs{B},\\bs{\\alpha},\\bs{\\Gamma}|\\bs{\\Pi}, \\bs{\\omega})] &=\n",
    "E_q\\big[ \\sum_j \\alpha_j \\big( \\sum_p \\gamma_{jp} \\{\\log(\\pi_p) - \\frac{R}{2}\\log(2\\pi) - \\frac{1}{2}\\log \\det(\\bs{V}_p) - \\frac{1}{2} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j\\} \\big) \\big | \\bs{\\alpha}, \\bs{\\Gamma}\\big] \\\\\n",
    "& = \\sum_j \\sum_p \\tilde{\\omega}_j \\tilde{\\pi}_{jp} \\{ \\log(\\pi_p) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\bs{V}_p) \\} - \\frac{1}{2} \\sum_j \\sum_p \\tilde{\\omega}_j \\tilde{\\pi}_{jp} E_q[\\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j|\\alpha_j, \\gamma_{jp}],\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\log p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}|\\bs{\\omega}, \\bs{\\Pi})] &=\n",
    "E_q\\big[ \\sum_j \\alpha_j \\big( \\sum_p \\gamma_{jp} \\{\\log(\\pi_p) - \\frac{R}{2}\\log(2\\pi) - \\frac{1}{2}\\log \\det(\\bs{V}_p) - \\frac{1}{2} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j\\} \\big) | \\bs{\\omega}, \\bs{\\Pi}\\big] \\\\\n",
    "& = E_q\\big[ \\sum_j \\sum_p \\alpha_j \\gamma_{jp} \\{ \\log(\\pi_p) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\bs{V}_p) \\} - \\frac{1}{2} \\sum_j \\alpha_j (\\bs{V}_p \\gamma_{jp} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j) \\big] \\\\\n",
    "& = \\sum_j \\sum_p \\tilde{\\omega}_j \\tilde{\\pi}_{jp} \\{ \\log(\\pi_p) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\bs{V}_p) \\} - \\frac{1}{2} \\sum_j \\sum_p E_q[\\alpha_j \\gamma_{jp} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j],\n",
    "\\end{align}\n",
    "\n",
    "where posterior weight $\\tilde{\\pi}_{jp}$ is estimated by $\\hat{\\tilde{\\pi}}_{jp}$ which has been derived previously, and $\\tilde{\\omega}_j$ is estimated Bayes Factor averaging, $\\hat{\\tilde{\\omega}}_j = \\frac{\\omega_j \\text{BF}_j}{\\sum_j \\omega_j \\text{BF}_j}$, where $\\text{BF}_j$ has also been derived previously.\n",
    "\n",
    "Now we are left to work on $E_q[\\alpha_j \\gamma_{jp} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j]$ which is a scalar,\n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\alpha_j \\gamma_{jp} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j] &= \n",
    "E_q\\big[E_q[\\alpha_j \\gamma_{jp} \\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j | \\alpha_j,\\gamma_{jp}] \\big] \\\\\n",
    "&= E_q\\big[E_q[\\alpha_j \\gamma_{jp} \\tr(\\bs{b}_j^\\intercal \\bs{V}_p^{-1} \\bs{b}_j) | \\alpha_j,\\gamma_{jp}] \\big] \\\\\n",
    "&= E_q\\big[E_q[\\alpha_j \\gamma_{jp} \\tr(\\bs{V}_p^{-1} \\bs{b}_j \\bs{b}_j^\\intercal) | \\alpha_j,\\gamma_{jp}] \\big] \\\\\n",
    "&= \\tr \\big \\{ E_q \\big [ \\bs{V}_p^{-1} E_q[\\alpha_j \\gamma_{jp} \\bs{b}_j \\bs{b}_j^\\intercal | \\alpha_j \\gamma_{jp}] \\big ] \\big \\} \\\\\n",
    "&= \\tr \\big \\{ E_q \\big [ \\bs{V}_p^{-1} \\tilde{\\omega}_j \\tilde{\\pi}_{jp} ({\\bar{\\bs{b}}_j}_p {\\bar{\\bs{b}}_j}_p^\\intercal + \\bs{U}_{jp})] \\big ] \\big \\} \\\\\n",
    "&= \\tilde{\\omega}_j\\tilde{\\pi}_{jp} \\tr[\\bs{V}_p^{-1}({\\bar{\\bs{b}}_j}_p {\\bar{\\bs{b}}_j}_p^\\intercal + \\bs{U}_{jp})]\n",
    "\\end{align}\n",
    "\n",
    "Hence,\n",
    "\\begin{align}\n",
    "E_q[\\log p(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}|\\bs{V}, \\bs{\\omega}, \\bs{\\pi})] &= \n",
    "\\sum_j \\sum_p \\tilde{\\omega}_j \\tilde{\\pi}_{jp} \\{ \\log(\\pi_p) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det (\\bs{V}_p) \\} -\n",
    "\\frac{1}{2} \\sum_j \\sum_p \\tilde{\\omega}_j\\tilde{\\pi}_{jp} \\tr[\\bs{V}_p^{-1}({\\bar{\\bs{b}}_j}_p {\\bar{\\bs{b}}_j}_p^\\intercal + \\bs{U}_{jp})]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $E_q[\\log q(\\bs{b}_j, \\bs{\\alpha}, \\bs{\\gamma}_j)]$\n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\log q(\\bs{b}_j, \\bs{\\alpha}, \\bs{\\gamma}_j)] & = \n",
    "E_q\\big[\\alpha_j \\sum_p \\gamma_{jp} \\{ \\log(\\tilde{\\pi}_{jp}) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(\\bs{U}_{jp}) - \\frac{1}{2} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)^\\intercal \\bs{U}_{jp}^{-1} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p) \\} \\big] \\\\\n",
    "& = \\tilde{\\omega}_j \\sum_p \\tilde{\\pi}_{jp} \\{ \\log(\\tilde{\\pi}_{jp}) - \\frac{R}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(\\bs{U}_{jp}) \\} -\n",
    "\\frac{1}{2} \\sum_p E_q\\big[ \\alpha_j \\gamma_{jp} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)^\\intercal \\bs{U}_{jp}^{-1} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)\\big],\n",
    "\\end{align}\n",
    "\n",
    "where $\\bs{b}_j | \\alpha_j, \\gamma_{jp} \\sim N_R({\\bar{\\bs{b}}_j}_p, \\bs{U}_{jp})$ have been derived previously, and the expectation\n",
    "\n",
    "\\begin{align}\n",
    "E_q\\big[ \\alpha_j \\gamma_{jp} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)^\\intercal \\bs{U}_{jp}^{-1} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)\\big] &= \n",
    "E_q \\big [ E_q[\\alpha_j \\gamma_{jp} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)^\\intercal \\bs{U}_{jp}^{-1} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p) | \\alpha_j, \\gamma_{jp} ] \\big] \\\\\n",
    "&= E_q \\big [ \\alpha_j \\gamma_{jp} E_q[(\\bs{b}_j - {\\bar{\\bs{b}}_j}_p)^\\intercal \\bs{U}_{jp}^{-1} (\\bs{b}_j - {\\bar{\\bs{b}}_j}_p) | \\alpha_j, \\gamma_{jp} ] \\big] \\\\\n",
    "&= E_q[R\\alpha_j\\gamma_{jp}] \\quad \\text{by recognizing the kernel of } \\bs{b}_j | \\alpha_j, \\gamma_{jp} \\sim N_R({\\bar{\\bs{b}}_j}_p, \\bs{U}_{jp})\\\\ \n",
    "&= R\\tilde{\\omega}_j\\tilde{\\pi}_{jp}\n",
    "\\end{align}\n",
    "\n",
    "Hence \n",
    "\n",
    "\\begin{align}\n",
    "E_q[\\log q(\\bs{B}, \\bs{\\alpha}, \\bs{\\Gamma}|\\bs{V}, \\bs{\\omega}, \\bs{\\pi})] &=\n",
    "\\sum_j \\tilde{\\omega}_j \\sum_p\\tilde{\\pi}_{jp} \\big( \\log(\\tilde{\\pi}_{jp}) - \\frac{R}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\bs{U}_{jp}) - \\frac{R}{2}\\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $E_q[\\log\\frac{\\prod_{l}p(\\bs{B}_l, \\bs{\\alpha}_l, \\bs{\\Gamma}_l | \\bs{\\Pi}_l, \\bs{\\omega}_l)}{\\prod_l \\prod_j q({\\bs{b}_l}_j, \\bs{\\alpha}_l, {\\bs{\\gamma}_l}_j)}]$, method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [],
   "panel": {
    "displayed": false,
    "height": 0,
    "style": "side"
   },
   "version": "0.9.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
